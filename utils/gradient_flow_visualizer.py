"""
æ¢¯åº¦æµå¯è§†åŒ–å·¥å…·ï¼šç†è§£å’Œåˆ†ææ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦ä¼ æ’­æœºåˆ¶
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from collections import defaultdict


class GradientFlowVisualizer:
    """æ¢¯åº¦æµå¯è§†åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¢¯åº¦æµå¯è§†åŒ–å™¨"""
        self.gradient_history = defaultdict(list)
        self.activation_history = defaultdict(list)
        self.weight_history = defaultdict(list)
        
    def create_sample_networks(self) -> Dict[str, nn.Module]:
        """åˆ›å»ºä¸åŒç±»å‹çš„ç¤ºä¾‹ç½‘ç»œ"""
        networks = {}
        
        # 1. ç®€å•çš„æ·±åº¦ç½‘ç»œ
        class DeepNetwork(nn.Module):
            def __init__(self, layer_sizes, activation='relu'):
                super().__init__()
                self.layers = nn.ModuleList()
                self.activation = activation
                
                for i in range(len(layer_sizes) - 1):
                    self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
            
            def forward(self, x):
                for i, layer in enumerate(self.layers):
                    x = layer(x)
                    if i < len(self.layers) - 1:  # ä¸åœ¨æœ€åä¸€å±‚æ¿€æ´»
                        if self.activation == 'relu':
                            x = F.relu(x)
                        elif self.activation == 'tanh':
                            x = torch.tanh(x)
                        elif self.activation == 'sigmoid':
                            x = torch.sigmoid(x)
                return x
        
        # 2. å¸¦æ®‹å·®è¿æ¥çš„ç½‘ç»œ
        class ResidualNetwork(nn.Module):
            def __init__(self, layer_sizes):
                super().__init__()
                self.layers = nn.ModuleList()
                self.shortcuts = nn.ModuleList()
                
                for i in range(len(layer_sizes) - 1):
                    self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))
                    if i > 0 and layer_sizes[i] == layer_sizes[i+1]:
                        self.shortcuts.append(nn.Identity())
                    else:
                        self.shortcuts.append(None)
            
            def forward(self, x):
                for i, layer in enumerate(self.layers):
                    out = layer(x)
                    if self.shortcuts[i] is not None:
                        out = out + x
                    x = F.relu(out)
                return x
        
        # 3. LSTMç½‘ç»œ
        class LSTMLayer(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers=2):
                super().__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
                self.output_layer = nn.Linear(hidden_size, hidden_size)
            
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                return self.output_layer(lstm_out)
        
        # åˆ›å»ºç½‘ç»œå®ä¾‹
        networks['deep_relu'] = DeepNetwork([512, 256, 128, 64, 32, 16, 8, 4, 2], 'relu')
        networks['deep_tanh'] = DeepNetwork([512, 256, 128, 64, 32, 16, 8, 4, 2], 'tanh')
        networks['deep_sigmoid'] = DeepNetwork([512, 256, 128, 64, 32, 16, 8, 4, 2], 'sigmoid')
        networks['residual'] = ResidualNetwork([512, 256, 256, 128, 128, 64, 64, 32])
        networks['lstm'] = LSTMLayer(512, 256, 3)
        
        return networks
    
    def analyze_gradient_flow(self, network: nn.Module, input_size: Tuple[int, int], 
                            num_batches: int = 10) -> Dict:
        """åˆ†æç½‘ç»œçš„æ¢¯åº¦æµ"""
        network.train()
        
        gradient_stats = defaultdict(list)
        activation_stats = defaultdict(list)
        
        for batch in range(num_batches):
            # ç”Ÿæˆéšæœºè¾“å…¥
            x = torch.randn(input_size[0], input_size[1])
            target = torch.randn(input_size[0], 1)
            
            # å‰å‘ä¼ æ’­
            network.zero_grad()
            output = network(x)
            
            # è®°å½•æ¿€æ´»å€¼
            self._record_activations(network, activation_stats, batch)
            
            # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
            loss = F.mse_loss(output.mean(dim=1, keepdim=True), target)
            loss.backward()
            
            # è®°å½•æ¢¯åº¦
            self._record_gradients(network, gradient_stats, batch)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        gradient_analysis = self._compute_gradient_stats(gradient_stats)
        activation_analysis = self._compute_activation_stats(activation_stats)
        
        return {
            'gradient_stats': gradient_analysis,
            'activation_stats': activation_analysis,
            'network_type': type(network).__name__
        }
    
    def _record_activations(self, network: nn.Module, stats: Dict, batch: int):
        """è®°å½•æ¿€æ´»å€¼"""
        for name, module in network.named_modules():
            if isinstance(module, (nn.Linear, nn.LSTM)):
                if hasattr(module, 'output') and module.output is not None:
                    activation = module.output.detach()
                    stats[name].append({
                        'batch': batch,
                        'mean': activation.mean().item(),
                        'std': activation.std().item(),
                        'min': activation.min().item(),
                        'max': activation.max().item(),
                        'shape': activation.shape
                    })
    
    def _record_gradients(self, network: nn.Module, stats: Dict, batch: int):
        """è®°å½•æ¢¯åº¦"""
        for name, param in network.named_parameters():
            if param.grad is not None:
                grad = param.grad.detach()
                stats[name].append({
                    'batch': batch,
                    'mean': grad.mean().item(),
                    'std': grad.std().item(),
                    'norm': grad.norm().item(),
                    'max': grad.max().item(),
                    'min': grad.min().item()
                })
    
    def _compute_gradient_stats(self, gradient_data: Dict) -> Dict:
        """è®¡ç®—æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, batches in gradient_data.items():
            if not batches:
                continue
            
            # æå–å„æ‰¹æ¬¡çš„æ¢¯åº¦èŒƒæ•°
            norms = [b['norm'] for b in batches]
            means = [b['mean'] for b in batches]
            stds = [b['std'] for b in batches]
            
            stats[name] = {
                'avg_norm': np.mean(norms),
                'std_norm': np.std(norms),
                'min_norm': np.min(norms),
                'max_norm': np.max(norms),
                'avg_mean': np.mean(means),
                'avg_std': np.mean(stds),
                'gradient_health': self._assess_gradient_health(norms)
            }
        
        return stats
    
    def _compute_activation_stats(self, activation_data: Dict) -> Dict:
        """è®¡ç®—æ¿€æ´»å€¼ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for name, batches in activation_data.items():
            if not batches:
                continue
            
            means = [b['mean'] for b in batches]
            stds = [b['std'] for b in batches]
            
            stats[name] = {
                'avg_mean': np.mean(means),
                'avg_std': np.std(stds),
                'activation_health': self._assess_activation_health(means, stds)
            }
        
        return stats
    
    def _assess_gradient_health(self, norms: List[float]) -> str:
        """è¯„ä¼°æ¢¯åº¦å¥åº·çŠ¶å†µ"""
        if len(norms) == 0:
            return "æ— æ•°æ®"
        
        avg_norm = np.mean(norms)
        min_norm = np.min(norms)
        
        if min_norm < 1e-7:
            return "æ¢¯åº¦æ¶ˆå¤±"
        elif avg_norm > 10:
            return "æ¢¯åº¦çˆ†ç‚¸"
        elif np.std(norms) / avg_norm > 2:
            return "æ¢¯åº¦ä¸ç¨³å®š"
        else:
            return "å¥åº·"
    
    def _assess_activation_health(self, means: List[float], stds: List[float]) -> str:
        """è¯„ä¼°æ¿€æ´»å€¼å¥åº·çŠ¶å†µ"""
        if len(means) == 0:
            return "æ— æ•°æ®"
        
        avg_mean = np.abs(np.mean(means))
        avg_std = np.mean(stds)
        
        if avg_mean < 0.01 and avg_std < 0.01:
            return "æ¿€æ´»é¥±å’Œ"
        elif avg_std < 0.1:
            return "æ¿€æ´»ç¨€ç–"
        else:
            return "æ­£å¸¸"
    
    def visualize_gradient_flow(self, network_name: str = 'deep_relu') -> go.Figure:
        """å¯è§†åŒ–æ¢¯åº¦æµ"""
        networks = self.create_sample_networks()
        
        if network_name not in networks:
            network_name = 'deep_relu'
        
        network = networks[network_name]
        
        # åˆ†ææ¢¯åº¦æµ
        input_size = (32, 512)  # batch_size=32, input_dim=512
        analysis = self.analyze_gradient_flow(network, input_size)
        
        # æå–æ¢¯åº¦èŒƒæ•°
        layer_names = []
        gradient_norms = []
        gradient_health = []
        
        for name, stats in analysis['gradient_stats'].items():
            if 'weight' in name:  # åªçœ‹æƒé‡å‚æ•°
                layer_names.append(name.replace('.weight', ''))
                gradient_norms.append(stats['avg_norm'])
                gradient_health.append(stats['gradient_health'])
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = go.Figure()
        
        # æ·»åŠ æ¢¯åº¦èŒƒæ•°çº¿
        fig.add_trace(go.Scatter(
            x=list(range(len(layer_names))),
            y=gradient_norms,
            mode='lines+markers',
            name='æ¢¯åº¦èŒƒæ•°',
            line=dict(width=3),
            marker=dict(size=8)
        ))
        
        # æ·»åŠ å¥åº·çŠ¶å†µæ ‡è®°
        health_colors = {'å¥åº·': 'green', 'æ¢¯åº¦æ¶ˆå¤±': 'red', 'æ¢¯åº¦çˆ†ç‚¸': 'orange', 'æ¢¯åº¦ä¸ç¨³å®š': 'purple'}
        for i, health in enumerate(gradient_health):
            if health != 'å¥åº·':
                fig.add_annotation(
                    x=i, y=gradient_norms[i],
                    text=health,
                    showarrow=True,
                    arrowhead=2,
                    arrowsize=1,
                    arrowwidth=2,
                    arrowcolor=health_colors.get(health, 'black'),
                    ax=0, ay=-40
                )
        
        fig.update_layout(
            title=f'æ¢¯åº¦æµåˆ†æ - {network_name}',
            xaxis_title='ç½‘ç»œå±‚ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰',
            yaxis_title='æ¢¯åº¦èŒƒæ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰',
            yaxis_type='log',
            height=500,
            xaxis=dict(tickmode='array', tickvals=list(range(len(layer_names))), ticktext=layer_names)
        )
        
        return fig
    
    def compare_activation_functions(self) -> go.Figure:
        """å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦æµ"""
        networks = self.create_sample_networks()
        input_size = (32, 512)
        
        fig = go.Figure()
        
        activation_functions = ['relu', 'tanh', 'sigmoid']
        colors = ['blue', 'red', 'green']
        
        for i, activation in enumerate(activation_functions):
            network_name = f'deep_{activation}'
            if network_name in networks:
                network = networks[network_name]
                analysis = self.analyze_gradient_flow(network, input_size)
                
                # æå–æ¢¯åº¦èŒƒæ•°
                gradient_norms = []
                for name, stats in analysis['gradient_stats'].items():
                    if 'weight' in name:
                        gradient_norms.append(stats['avg_norm'])
                
                fig.add_trace(go.Scatter(
                    x=list(range(len(gradient_norms))),
                    y=gradient_norms,
                    mode='lines+markers',
                    name=activation.upper(),
                    line=dict(color=colors[i], width=2),
                    marker=dict(size=6)
                ))
        
        fig.update_layout(
            title='ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦æµå¯¹æ¯”',
            xaxis_title='ç½‘ç»œå±‚æ·±åº¦',
            yaxis_title='æ¢¯åº¦èŒƒæ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰',
            yaxis_type='log',
            height=500,
            legend=dict(x=0.02, y=0.98)
        )
        
        return fig
    
    def visualize_residual_connections(self) -> go.Figure:
        """å¯è§†åŒ–æ®‹å·®è¿æ¥å¯¹æ¢¯åº¦æµçš„å½±å“"""
        networks = self.create_sample_networks()
        input_size = (32, 512)
        
        fig = go.Figure()
        
        # å¯¹æ¯”æœ‰æ®‹å·®è¿æ¥å’Œæ²¡æœ‰æ®‹å·®è¿æ¥çš„ç½‘ç»œ
        network_types = ['deep_relu', 'residual']
        labels = ['æ— æ®‹å·®è¿æ¥', 'æœ‰æ®‹å·®è¿æ¥']
        colors = ['red', 'green']
        
        for i, (network_type, label) in enumerate(zip(network_types, labels)):
            if network_type in networks:
                network = networks[network_type]
                analysis = self.analyze_gradient_flow(network, input_size)
                
                # æå–æ¢¯åº¦èŒƒæ•°
                gradient_norms = []
                for name, stats in analysis['gradient_stats'].items():
                    if 'weight' in name:
                        gradient_norms.append(stats['avg_norm'])
                
                fig.add_trace(go.Scatter(
                    x=list(range(len(gradient_norms))),
                    y=gradient_norms,
                    mode='lines+markers',
                    name=label,
                    line=dict(color=colors[i], width=2),
                    marker=dict(size=6)
                ))
        
        fig.update_layout(
            title='æ®‹å·®è¿æ¥å¯¹æ¢¯åº¦æµçš„å½±å“',
            xaxis_title='ç½‘ç»œå±‚æ·±åº¦',
            yaxis_title='æ¢¯åº¦èŒƒæ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰',
            yaxis_type='log',
            height=500
        )
        
        return fig
    
    def create_gradient_flow_report(self) -> str:
        """ç”Ÿæˆæ¢¯åº¦æµåˆ†ææŠ¥å‘Š"""
        networks = self.create_sample_networks()
        input_size = (32, 512)
        
        report = "# æ¢¯åº¦æµåˆ†ææŠ¥å‘Š\n\n"
        
        # åˆ†ææ¯ç§ç½‘ç»œç±»å‹
        for network_name, network in networks.items():
            report += f"## {network_name.replace('_', ' ').title()}\n"
            
            analysis = self.analyze_gradient_flow(network, input_size)
            
            # ç»Ÿè®¡æ¢¯åº¦å¥åº·çŠ¶å†µ
            health_counts = {}
            for stats in analysis['gradient_stats'].values():
                health = stats['gradient_health']
                health_counts[health] = health_counts.get(health, 0) + 1
            
            report += "### æ¢¯åº¦å¥åº·çŠ¶å†µåˆ†å¸ƒ\n"
            for health, count in health_counts.items():
                report += f"- {health}: {count} å±‚\n"
            
            # æ‰¾å‡ºé—®é¢˜å±‚
            problem_layers = []
            for name, stats in analysis['gradient_stats'].items():
                if stats['gradient_health'] != 'å¥åº·':
                    problem_layers.append(f"{name}: {stats['gradient_health']}")
            
            if problem_layers:
                report += "### é—®é¢˜å±‚\n"
                for layer in problem_layers:
                    report += f"- {layer}\n"
            else:
                report += "### âœ… æ‰€æœ‰å±‚æ¢¯åº¦å¥åº·\n"
            
            report += "\n"
        
        # æ•™å­¦å†…å®¹
        report += """
## æ¢¯åº¦æµé—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### ğŸ”´ å¸¸è§é—®é¢˜

#### 1. æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradients)
**ç—‡çŠ¶**: æµ…å±‚æ¢¯åº¦æ¥è¿‘é›¶ï¼Œæ·±å±‚ç½‘ç»œéš¾ä»¥è®­ç»ƒ
**åŸå› **: 
- sigmoid/tanhæ¿€æ´»å‡½æ•°çš„å¯¼æ•°åœ¨è¾“å…¥ç»å¯¹å€¼å¤§æ—¶æ¥è¿‘0
- ç½‘ç»œè¿‡æ·±ï¼Œæ¢¯åº¦è¿ä¹˜å¯¼è‡´æŒ‡æ•°è¡°å‡
- æƒé‡åˆå§‹åŒ–ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ReLUç­‰éé¥±å’Œæ¿€æ´»å‡½æ•°
- æ®‹å·®è¿æ¥ (ResNet)
- æ‰¹å½’ä¸€åŒ– (Batch Normalization)
- åˆé€‚çš„æƒé‡åˆå§‹åŒ– (Xavier/Kaiming)

#### 2. æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradients)
**ç—‡çŠ¶**: æ¢¯åº¦å€¼è¿‡å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š
**åŸå› **:
- å­¦ä¹ ç‡è¿‡é«˜
- æƒé‡åˆå§‹åŒ–æ–¹å·®è¿‡å¤§
- RNNä¸­çš„é•¿æ—¶é—´ä¾èµ–

**è§£å†³æ–¹æ¡ˆ**:
- æ¢¯åº¦è£å‰ª (Gradient Clipping)
- é™ä½å­¦ä¹ ç‡
- æƒé‡æ­£åˆ™åŒ–
- LSTM/GRUç»“æ„

#### 3. æ¢¯åº¦ä¸ç¨³å®š (Unstable Gradients)
**ç—‡çŠ¶**: æ¢¯åº¦æ–¹å·®å¤§ï¼Œè®­ç»ƒéœ‡è¡
**åŸå› **:
- æ‰¹å¤§å°è¿‡å°
- å­¦ä¹ ç‡è°ƒåº¦ä¸å½“
- æ•°æ®é¢„å¤„ç†é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
- å¢å¤§æ‰¹å¤§å°
- å­¦ä¹ ç‡é¢„çƒ­ (Warmup)
- æ¢¯åº¦ç´¯ç§¯
- è‡ªé€‚åº”ä¼˜åŒ–å™¨ (Adam, RMSprop)

### ğŸ¯ æœ€ä½³å®è·µ

1. **æ¿€æ´»å‡½æ•°é€‰æ‹©**
   - æ·±åº¦ç½‘ç»œä¼˜å…ˆä½¿ç”¨ReLUåŠå…¶å˜ä½“
   - è¾“å‡ºå±‚æ ¹æ®ä»»åŠ¡é€‰æ‹©åˆé€‚çš„æ¿€æ´»å‡½æ•°
   - æ³¨æ„ReLUçš„"æ­»äº¡"é—®é¢˜

2. **ç½‘ç»œè®¾è®¡**
   - æ·±å±‚ç½‘ç»œè€ƒè™‘æ®‹å·®è¿æ¥
   - ä½¿ç”¨æ‰¹å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ
   - åˆç†çš„ç½‘ç»œæ·±åº¦ï¼Œé¿å…è¿‡æ·±

3. **è®­ç»ƒæŠ€å·§**
   - ç›‘æ§æ¢¯åº¦èŒƒæ•°å˜åŒ–
   - ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
   - é€‚å½“çš„å­¦ä¹ ç‡è°ƒåº¦

4. **è¯Šæ–­å·¥å…·**
   - å®šæœŸæ£€æŸ¥æ¢¯åº¦åˆ†å¸ƒ
   - ç›‘æ§æ¿€æ´»å€¼èŒƒå›´
   - ä½¿ç”¨TensorBoardç­‰å¯è§†åŒ–å·¥å…·
"""
        
        return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    visualizer = GradientFlowVisualizer()
    
    # åˆ›å»ºç¤ºä¾‹ç½‘ç»œ
    networks = visualizer.create_sample_networks()
    print(f"åˆ›å»ºäº† {len(networks)} ç§ä¸åŒç±»å‹çš„ç½‘ç»œ")
    
    # åˆ†ææ¢¯åº¦æµ
    network = networks['deep_relu']
    analysis = visualizer.analyze_gradient_flow(network, (32, 512))
    print(f"åˆ†æäº† {len(analysis['gradient_stats'])} ä¸ªå‚æ•°çš„æ¢¯åº¦")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = visualizer.create_gradient_flow_report()
    print(report)
