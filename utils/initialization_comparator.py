"""
å‚æ•°åˆå§‹åŒ–å¯¹æ¯”å·¥å…·ï¼šæ¼”ç¤ºä¸åŒåˆå§‹åŒ–æ–¹æ³•å¯¹æ¨¡å‹è®­ç»ƒçš„å½±å“
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from scipy import stats


class InitializationComparator:
    """å‚æ•°åˆå§‹åŒ–æ–¹æ³•å¯¹æ¯”å™¨"""
    
    def __init__(self, layer_sizes: List[int] = [512, 256, 128, 64, 10]):
        """
        Args:
            layer_sizes: ç½‘ç»œå„å±‚çš„å¤§å°
        """
        self.layer_sizes = layer_sizes
        self.init_methods = {
            "xavier_uniform": self._xavier_uniform_init,
            "xavier_normal": self._xavier_normal_init,
            "kaiming_uniform": self._kaiming_uniform_init,
            "kaiming_normal": self._kaiming_normal_init,
            "orthogonal": self._orthogonal_init,
            "lecun_normal": self._lecun_normal_init,
            "random_normal": self._random_normal_init,
            "random_uniform": self._random_uniform_init
        }
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.init_results = {}
        
    def _xavier_uniform_init(self, weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:
        """Xavierå‡åŒ€åˆå§‹åŒ–"""
        return nn.init.xavier_uniform_(weight, gain=gain)
    
    def _xavier_normal_init(self, weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:
        """Xavieræ­£æ€åˆå§‹åŒ–"""
        return nn.init.xavier_normal_(weight, gain=gain)
    
    def _kaiming_uniform_init(self, weight: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu') -> torch.Tensor:
        """Kaimingå‡åŒ€åˆå§‹åŒ–"""
        return nn.init.kaiming_uniform_(weight, a=a, mode=mode, nonlinearity=nonlinearity)
    
    def _kaiming_normal_init(self, weight: torch.Tensor, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu') -> torch.Tensor:
        """Kaimingæ­£æ€åˆå§‹åŒ–"""
        return nn.init.kaiming_normal_(weight, a=a, mode=mode, nonlinearity=nonlinearity)
    
    def _orthogonal_init(self, weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:
        """æ­£äº¤åˆå§‹åŒ–"""
        return nn.init.orthogonal_(weight, gain=gain)
    
    def _lecun_normal_init(self, weight: torch.Tensor) -> torch.Tensor:
        """LeCunæ­£æ€åˆå§‹åŒ–"""
        fan_in = weight.size(1)
        std = np.sqrt(1.0 / fan_in)
        return nn.init.normal_(weight, 0, std)
    
    def _random_normal_init(self, weight: torch.Tensor, mean: float = 0.0, std: float = 0.02) -> torch.Tensor:
        """éšæœºæ­£æ€åˆå§‹åŒ–"""
        return nn.init.normal_(weight, mean, std)
    
    def _random_uniform_init(self, weight: torch.Tensor, a: float = -0.05, b: float = 0.05) -> torch.Tensor:
        """éšæœºå‡åŒ€åˆå§‹åŒ–"""
        return nn.init.uniform_(weight, a, b)
    
    def create_sample_network(self) -> nn.Module:
        """åˆ›å»ºç¤ºä¾‹ç½‘ç»œ"""
        layers = []
        for i in range(len(self.layer_sizes) - 1):
            layers.append(nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]))
            if i < len(self.layer_sizes) - 2:  # ä¸åœ¨æœ€åä¸€å±‚æ·»åŠ æ¿€æ´»
                layers.append(nn.ReLU())
        
        return nn.Sequential(*layers)
    
    def analyze_initialization(self, init_method: str, n_samples: int = 1000) -> Dict:
        """åˆ†æç‰¹å®šåˆå§‹åŒ–æ–¹æ³•"""
        if init_method not in self.init_methods:
            raise ValueError(f"æœªçŸ¥çš„åˆå§‹åŒ–æ–¹æ³•: {init_method}")
        
        # åˆ›å»ºç½‘ç»œ
        net = self.create_sample_network()
        
        # åº”ç”¨åˆå§‹åŒ–
        init_func = self.init_methods[init_method]
        for module in net.modules():
            if isinstance(module, nn.Linear):
                init_func(module.weight)
                nn.init.zeros_(module.bias)
        
        # ç”Ÿæˆéšæœºè¾“å…¥
        x = torch.randn(n_samples, self.layer_sizes[0])
        
        # å‰å‘ä¼ æ’­åˆ†æ
        activations = {}
        with torch.no_grad():
            current_input = x
            layer_idx = 0
            
            for i, module in enumerate(net.modules()):
                if isinstance(module, nn.Linear):
                    # è®°å½•è¾“å…¥åˆ†å¸ƒ
                    activations[f'layer_{layer_idx}_input'] = {
                        'mean': current_input.mean().item(),
                        'std': current_input.std().item(),
                        'min': current_input.min().item(),
                        'max': current_input.max().item(),
                        'shape': current_input.shape
                    }
                    
                    # å‰å‘ä¼ æ’­
                    output = module(current_input)
                    
                    # è®°å½•è¾“å‡ºåˆ†å¸ƒ
                    activations[f'layer_{layer_idx}_output'] = {
                        'mean': output.mean().item(),
                        'std': output.std().item(),
                        'min': output.min().item(),
                        'max': output.max().item(),
                        'shape': output.shape
                    }
                    
                    # è®°å½•æƒé‡åˆ†å¸ƒ
                    weight = module.weight
                    activations[f'layer_{layer_idx}_weight'] = {
                        'mean': weight.mean().item(),
                        'std': weight.std().item(),
                        'min': weight.min().item(),
                        'max': weight.max().item(),
                        'shape': weight.shape,
                        'frobenius_norm': weight.norm().item()
                    }
                    
                    current_input = output
                    layer_idx += 1
                
                elif isinstance(module, nn.ReLU):
                    current_input = module(current_input)
        
        # è®¡ç®—æ¢¯åº¦æµï¼ˆæ¨¡æ‹Ÿï¼‰
        gradient_stats = self._analyze_gradient_flow(net, x)
        
        return {
            'method': init_method,
            'activations': activations,
            'gradient_stats': gradient_stats,
            'network': net
        }
    
    def _analyze_gradient_flow(self, net: nn.Module, x: torch.Tensor) -> Dict:
        """åˆ†ææ¢¯åº¦æµ"""
        # åˆ›å»ºæŸå¤±å’Œåå‘ä¼ æ’­
        output = net(x)
        loss = output.mean()
        loss.backward()
        
        gradient_stats = {}
        for name, param in net.named_parameters():
            if param.grad is not None:
                grad = param.grad
                gradient_stats[name] = {
                    'mean': grad.mean().item(),
                    'std': grad.std().item(),
                    'min': grad.min().item(),
                    'max': grad.max().item(),
                    'norm': grad.norm().item()
                }
        
        return gradient_stats
    
    def compare_all_initializations(self) -> Dict:
        """å¯¹æ¯”æ‰€æœ‰åˆå§‹åŒ–æ–¹æ³•"""
        results = {}
        
        for method in self.init_methods.keys():
            print(f"åˆ†æåˆå§‹åŒ–æ–¹æ³•: {method}")
            results[method] = self.analyze_initialization(method)
        
        self.init_results = results
        return results
    
    def visualize_weight_distributions(self) -> go.Figure:
        """å¯è§†åŒ–ä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„æƒé‡åˆ†å¸ƒ"""
        if not self.init_results:
            self.compare_all_initializations()
        
        fig = go.Figure()
        
        for method, result in self.init_results.items():
            # è·å–ç¬¬ä¸€å±‚çš„æƒé‡
            weight_key = f'layer_0_weight'
            if weight_key in result['activations']:
                weight_stats = result['activations'][weight_key]
                
                # ç”Ÿæˆæ¨¡æ‹Ÿåˆ†å¸ƒï¼ˆåŸºäºç»Ÿè®¡ä¿¡æ¯ï¼‰
                samples = np.random.normal(
                    weight_stats['mean'], 
                    weight_stats['std'], 
                    1000
                )
                
                fig.add_trace(go.Histogram(
                    x=samples,
                    name=method,
                    opacity=0.7,
                    nbinsx=50
                ))
        
        fig.update_layout(
            title='ä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„æƒé‡åˆ†å¸ƒå¯¹æ¯”ï¼ˆç¬¬ä¸€å±‚ï¼‰',
            xaxis_title='æƒé‡å€¼',
            yaxis_title='é¢‘æ¬¡',
            barmode='overlay',
            height=500
        )
        
        return fig
    
    def visualize_activation_evolution(self) -> go.Figure:
        """å¯è§†åŒ–æ¿€æ´»å€¼åœ¨å„å±‚çš„æ¼”åŒ–"""
        if not self.init_results:
            self.compare_all_initializations()
        
        fig = go.Figure()
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ–¹æ³•
        representative_methods = ['xavier_normal', 'kaiming_normal', 'random_normal']
        colors = ['blue', 'red', 'green']
        
        for i, method in enumerate(representative_methods):
            if method in self.init_results:
                result = self.init_results[method]
                
                # æ”¶é›†å„å±‚çš„æ¿€æ´»ç»Ÿè®¡
                layers = []
                means = []
                stds = []
                
                for key, stats in result['activations'].items():
                    if 'output' in key:
                        layer_num = int(key.split('_')[1])
                        layers.append(layer_num)
                        means.append(stats['mean'])
                        stds.append(stats['std'])
                
                # æ·»åŠ å‡å€¼çº¿
                fig.add_trace(go.Scatter(
                    x=layers,
                    y=means,
                    mode='lines+markers',
                    name=f'{method} (å‡å€¼)',
                    line=dict(color=colors[i], width=2),
                    legendgroup=method
                ))
                
                # æ·»åŠ æ ‡å‡†å·®åŒºåŸŸ
                upper = np.array(means) + np.array(stds)
                lower = np.array(means) - np.array(stds)
                
                fig.add_trace(go.Scatter(
                    x=layers + layers[::-1],
                    y=list(upper) + list(lower[::-1]),
                    fill='toself',
                    fillcolor=f'rgba({colors[i]},0.2)',
                    line=dict(color='rgba(255,255,255,0)'),
                    hoverinfo="skip",
                    showlegend=False,
                    legendgroup=method
                ))
        
        fig.update_layout(
            title='æ¿€æ´»å€¼åœ¨å„å±‚çš„æ¼”åŒ–',
            xaxis_title='å±‚æ•°',
            yaxis_title='æ¿€æ´»å€¼',
            height=500
        )
        
        return fig
    
    def visualize_gradient_flow(self) -> go.Figure:
        """å¯è§†åŒ–æ¢¯åº¦æµ"""
        if not self.init_results:
            self.compare_all_initializations()
        
        fig = go.Figure()
        
        for method, result in self.init_results.items():
            gradient_stats = result['gradient_stats']
            
            # æ”¶é›†æ¢¯åº¦èŒƒæ•°
            layer_names = []
            grad_norms = []
            
            for name, stats in gradient_stats.items():
                layer_names.append(name)
                grad_norms.append(stats['norm'])
            
            fig.add_trace(go.Scatter(
                x=list(range(len(grad_norms))),
                y=grad_norms,
                mode='lines+markers',
                name=method,
                line=dict(width=2)
            ))
        
        fig.update_layout(
            title='ä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„æ¢¯åº¦æµå¯¹æ¯”',
            xaxis_title='å±‚æ•°ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰',
            yaxis_title='æ¢¯åº¦èŒƒæ•°ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰',
            yaxis_type='log',
            height=500
        )
        
        return fig
    
    def create_initialization_report(self) -> str:
        """ç”Ÿæˆåˆå§‹åŒ–å¯¹æ¯”æŠ¥å‘Š"""
        if not self.init_results:
            self.compare_all_initializations()
        
        report = "# å‚æ•°åˆå§‹åŒ–æ–¹æ³•å¯¹æ¯”æŠ¥å‘Š\n\n"
        
        # æ–¹æ³•ä»‹ç»
        report += "## åˆå§‹åŒ–æ–¹æ³•ä»‹ç»\n\n"
        
        method_descriptions = {
            "xavier_uniform": "Xavierå‡åŒ€åˆå§‹åŒ– - é€‚ç”¨äºtanhæ¿€æ´»å‡½æ•°",
            "xavier_normal": "Xavieræ­£æ€åˆå§‹åŒ– - é€‚ç”¨äºtanhæ¿€æ´»å‡½æ•°",
            "kaiming_uniform": "Kaimingå‡åŒ€åˆå§‹åŒ– - é€‚ç”¨äºReLUæ¿€æ´»å‡½æ•°",
            "kaiming_normal": "Kaimingæ­£æ€åˆå§‹åŒ– - é€‚ç”¨äºReLUæ¿€æ´»å‡½æ•°",
            "orthogonal": "æ­£äº¤åˆå§‹åŒ– - ä¿æŒæ¢¯åº¦èŒƒæ•°ç¨³å®š",
            "lecun_normal": "LeCunæ­£æ€åˆå§‹åŒ– - é€‚ç”¨äºSELUæ¿€æ´»å‡½æ•°",
            "random_normal": "éšæœºæ­£æ€åˆå§‹åŒ– - ç®€å•çš„åŸºå‡†æ–¹æ³•",
            "random_uniform": "éšæœºå‡åŒ€åˆå§‹åŒ– - ç®€å•çš„åŸºå‡†æ–¹æ³•"
        }
        
        for method, desc in method_descriptions.items():
            report += f"### {method}\n{desc}\n\n"
        
        # æ€§èƒ½å¯¹æ¯”
        report += "## æ€§èƒ½å¯¹æ¯”åˆ†æ\n\n"
        
        for method, result in self.init_results.items():
            report += f"### {method}\n"
            
            # åˆ†ææ¿€æ´»å€¼ç¨³å®šæ€§
            output_means = []
            output_stds = []
            
            for key, stats in result['activations'].items():
                if 'output' in key:
                    output_means.append(stats['mean'])
                    output_stds.append(stats['std'])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
            grad_norms = [stats['norm'] for stats in result['gradient_stats'].values()]
            min_grad_norm = min(grad_norms)
            max_grad_norm = max(grad_norms)
            
            report += f"- æ¿€æ´»å€¼å‡å€¼èŒƒå›´: [{min(output_means):.4f}, {max(output_means):.4f}]\n"
            report += f"- æ¿€æ´»å€¼æ ‡å‡†å·®èŒƒå›´: [{min(output_stds):.4f}, {max(output_stds):.4f}]\n"
            report += f"- æ¢¯åº¦èŒƒæ•°èŒƒå›´: [{min_grad_norm:.6f}, {max_grad_norm:.6f}]\n"
            
            # åˆ¤æ–­é—®é¢˜
            if min_grad_norm < 1e-6:
                report += "- âš ï¸ å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é£é™©\n"
            elif max_grad_norm > 10:
                report += "- âš ï¸ å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸é£é™©\n"
            else:
                report += "- âœ… æ¢¯åº¦æµç›¸å¯¹ç¨³å®š\n"
            
            report += "\n"
        
        # æ¨èå»ºè®®
        report += "## æ¨èå»ºè®®\n\n"
        report += """
### ğŸ¯ æ ¹æ®æ¿€æ´»å‡½æ•°é€‰æ‹©åˆå§‹åŒ–æ–¹æ³•
- **ReLU/LeakyReLU**: æ¨èä½¿ç”¨Kaimingåˆå§‹åŒ–
- **tanh/sigmoid**: æ¨èä½¿ç”¨Xavieråˆå§‹åŒ–  
- **SELU**: æ¨èä½¿ç”¨LeCunåˆå§‹åŒ–

### ğŸš€ ç‰¹æ®Šåœºæ™¯æ¨è
- **æ·±åº¦ç½‘ç»œ**: æ­£äº¤åˆå§‹åŒ–æœ‰åŠ©äºä¿æŒæ¢¯åº¦ç¨³å®š
- **RNN/LSTM**: æ­£äº¤åˆå§‹åŒ–å¯¹å¾ªç¯é—¨å¾ˆé‡è¦
- **ç”Ÿæˆæ¨¡å‹**: æœ‰æ—¶éœ€è¦æ›´ä¿å®ˆçš„åˆå§‹åŒ–æ–¹å·®

### âš ï¸ å¸¸è§é™·é˜±
- é¿å…ä½¿ç”¨è¿‡å¤§çš„åˆå§‹åŒ–æ–¹å·®
- æ³¨æ„åç½®é¡¹çš„åˆå§‹åŒ–ï¼ˆé€šå¸¸è®¾ä¸º0ï¼‰
- æ‰¹å½’å½’åŒ–å±‚å¯ä»¥ç¼“è§£åˆå§‹åŒ–é—®é¢˜
"""
        
        return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    comparator = InitializationComparator([512, 256, 128, 64, 10])
    
    # å¯¹æ¯”æ‰€æœ‰åˆå§‹åŒ–æ–¹æ³•
    results = comparator.compare_all_initializations()
    print(f"å¯¹æ¯”äº† {len(results)} ç§åˆå§‹åŒ–æ–¹æ³•")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = comparator.create_initialization_report()
    print(report)
