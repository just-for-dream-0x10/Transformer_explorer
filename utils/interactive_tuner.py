"""
äº¤äº’å¼å‚æ•°è°ƒèŠ‚å·¥å…·ï¼šå®æ—¶çœ‹åˆ°å‚æ•°å˜åŒ–å¯¹æ¨¡å‹çš„å½±å“
"""
import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Tuple, Optional
import time


class InteractiveParameterTuner:
    """äº¤äº’å¼å‚æ•°è°ƒèŠ‚å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‚æ•°è°ƒèŠ‚å™¨"""
        self.model_cache = {}
        
    def create_simple_model(self, d_model: int, n_heads: int, n_layers: int, 
                           activation: str, dropout: float) -> nn.Module:
        """åˆ›å»ºç®€å•çš„Transformeræ¨¡å‹"""
        
        class SimpleTransformerBlock(nn.Module):
            def __init__(self, d_model, n_heads, dropout, activation):
                super().__init__()
                self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
                self.norm1 = nn.LayerNorm(d_model)
                self.norm2 = nn.LayerNorm(d_model)
                
                self.ffn = nn.Sequential(
                    nn.Linear(d_model, 4 * d_model),
                    nn.ReLU() if activation == 'relu' else nn.GELU(),
                    nn.Dropout(dropout),
                    nn.Linear(4 * d_model, d_model),
                    nn.Dropout(dropout)
                )
                
            def forward(self, x):
                # Self-attention
                attn_out, _ = self.attention(x, x, x)
                x = self.norm1(x + attn_out)
                
                # FFN
                ffn_out = self.ffn(x)
                x = self.norm2(x + ffn_out)
                
                return x
        
        class SimpleTransformer(nn.Module):
            def __init__(self, d_model, n_heads, n_layers, activation, dropout):
                super().__init__()
                self.layers = nn.ModuleList([
                    SimpleTransformerBlock(d_model, n_heads, dropout, activation)
                    for _ in range(n_layers)
                ])
                
            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)
                return x
        
        return SimpleTransformer(d_model, n_heads, n_layers, activation, dropout)
    
    def calculate_model_metrics(self, model: nn.Module, seq_len: int, batch_size: int = 8) -> Dict:
        """è®¡ç®—æ¨¡å‹æŒ‡æ ‡"""
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # ä¼°ç®—è®¡ç®—é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        d_model = model.layers[0].attention.embed_dim
        n_heads = model.layers[0].attention.num_heads
        n_layers = len(model.layers)
        
        # Self-attention FLOPs
        qkv_flops = 3 * batch_size * seq_len * d_model * d_model
        attn_flops = batch_size * n_heads * seq_len * seq_len * (d_model // n_heads)
        
        # FFN FLOPs
        ffn_flops = batch_size * seq_len * d_model * (4 * d_model) * 2
        
        total_flops = n_layers * (qkv_flops + attn_flops + ffn_flops)
        
        # ä¼°ç®—æ˜¾å­˜ï¼ˆç®€åŒ–ç‰ˆï¼‰
        param_memory = total_params * 4 / (1024**2)  # FP32
        activation_memory = batch_size * seq_len * d_model * 4 / (1024**2) * n_layers
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'total_flops': total_flops,
            'param_memory_mb': param_memory,
            'activation_memory_mb': activation_memory,
            'total_memory_mb': param_memory + activation_memory,
            'estimated_inference_time_ms': total_flops / 1e9 * 1000  # å‡è®¾1 TFLOPS
        }
    
    def simulate_forward_pass(self, model: nn.Module, seq_len: int, batch_size: int = 2) -> Dict:
        """æ¨¡æ‹Ÿå‰å‘ä¼ æ’­å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯"""
        model.eval()
        
        # åˆ›å»ºéšæœºè¾“å…¥
        x = torch.randn(batch_size, seq_len, model.layers[0].attention.embed_dim)
        
        with torch.no_grad():
            start_time = time.time()
            output = model(x)
            end_time = time.time()
            
            # æ”¶é›†å„å±‚çš„æ¿€æ´»ç»Ÿè®¡
            layer_stats = []
            for i, layer in enumerate(model.layers):
                # è®¡ç®—è¯¥å±‚çš„è¾“å‡ºèŒƒæ•°
                layer_output = layer(x)
                output_norm = layer_output.norm(dim=-1).mean().item()
                output_std = layer_output.std(dim=-1).mean().item()
                
                layer_stats.append({
                    'layer': i,
                    'output_norm': output_norm,
                    'output_std': output_std
                })
                
                x = layer_output
        
        return {
            'inference_time_ms': (end_time - start_time) * 1000,
            'output_shape': output.shape,
            'layer_stats': layer_stats
        }
    
    def create_parameter_impact_visualization(self, param_name: str, param_values: List[float], 
                                            base_config: Dict) -> go.Figure:
        """åˆ›å»ºå‚æ•°å½±å“å¯è§†åŒ–"""
        results = []
        
        for value in param_values:
            config = base_config.copy()
            config[param_name] = value
            
            # åˆ›å»ºæ¨¡å‹
            model = self.create_simple_model(
                config['d_model'], config['n_heads'], 
                config['n_layers'], config['activation'], config['dropout']
            )
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = self.calculate_model_metrics(model, config['seq_len'])
            
            results.append({
                param_name: value,
                'params_millions': metrics['total_params'] / 1e6,
                'flops_gflops': metrics['total_flops'] / 1e9,
                'memory_mb': metrics['total_memory_mb'],
                'inference_time_ms': metrics['estimated_inference_time_ms']
            })
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = go.Figure()
        
        # æ·»åŠ å‚æ•°é‡æ›²çº¿
        fig.add_trace(go.Scatter(
            x=[r[param_name] for r in results],
            y=[r['params_millions'] for r in results],
            mode='lines+markers',
            name='å‚æ•°é‡ (M)',
            yaxis='y'
        ))
        
        # æ·»åŠ FLOPsæ›²çº¿
        fig.add_trace(go.Scatter(
            x=[r[param_name] for r in results],
            y=[r['flops_gflops'] for r in results],
            mode='lines+markers',
            name='FLOPs (GFLOPs)',
            yaxis='y2'
        ))
        
        fig.update_layout(
            title=f'{param_name} å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“',
            xaxis_title=param_name,
            yaxis=dict(title='å‚æ•°é‡ (M)', side='left'),
            yaxis2=dict(title='FLOPs (GFLOPs)', side='right', overlaying='y'),
            height=500
        )
        
        return fig
    
    def create_attention_head_analysis(self, d_model: int, head_options: List[int]) -> go.Figure:
        """åˆ›å»ºæ³¨æ„åŠ›å¤´æ•°åˆ†æ"""
        results = []
        
        for n_heads in head_options:
            if d_model % n_heads != 0:
                continue
                
            model = self.create_simple_model(d_model, n_heads, 4, 'gelu', 0.1)
            metrics = self.calculate_model_metrics(model, 128)
            
            # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
            forward_stats = self.simulate_forward_pass(model, 128)
            
            results.append({
                'n_heads': n_heads,
                'head_dim': d_model // n_heads,
                'params_millions': metrics['total_params'] / 1e6,
                'inference_time_ms': forward_stats['inference_time_ms'],
                'memory_mb': metrics['total_memory_mb']
            })
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=[r['n_heads'] for r in results],
            y=[r['params_millions'] for r in results],
            mode='lines+markers',
            name='å‚æ•°é‡ (M)',
            text=[f"å¤´ç»´åº¦: {r['head_dim']}" for r in results],
            hovertemplate='æ³¨æ„åŠ›å¤´æ•°: %{x}<br>å‚æ•°é‡: %{y:.2f}M<br>%{text}<extra></extra>'
        ))
        
        fig.add_trace(go.Scatter(
            x=[r['n_heads'] for r in results],
            y=[r['inference_time_ms'] for r in results],
            mode='lines+markers',
            name='æ¨ç†æ—¶é—´ (ms)',
            yaxis='y2'
        ))
        
        fig.update_layout(
            title='æ³¨æ„åŠ›å¤´æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“',
            xaxis_title='æ³¨æ„åŠ›å¤´æ•°',
            yaxis=dict(title='å‚æ•°é‡ (M)', side='left'),
            yaxis2=dict(title='æ¨ç†æ—¶é—´ (ms)', side='right', overlaying='y'),
            height=500
        )
        
        return fig
    
    def create_depth_vs_width_analysis(self) -> go.Figure:
        """åˆ›å»ºæ·±åº¦vså®½åº¦åˆ†æ"""
        # ä¸åŒçš„é…ç½®
        configs = [
            {'n_layers': 2, 'd_model': 512, 'name': 'æµ…å±‚å®½æ¨¡å‹'},
            {'n_layers': 4, 'd_model': 256, 'name': 'ä¸­å±‚ä¸­ç­‰æ¨¡å‹'},
            {'n_layers': 8, 'd_model': 128, 'name': 'æ·±å±‚çª„æ¨¡å‹'},
            {'n_layers': 12, 'd_model': 86, 'name': 'å¾ˆæ·±å±‚å¾ˆçª„æ¨¡å‹'},
        ]
        
        results = []
        
        for config in configs:
            model = self.create_simple_model(
                config['d_model'], 8, config['n_layers'], 'gelu', 0.1
            )
            
            metrics = self.calculate_model_metrics(model, 128)
            forward_stats = self.simulate_forward_pass(model, 128)
            
            results.append({
                'name': config['name'],
                'n_layers': config['n_layers'],
                'd_model': config['d_model'],
                'params_millions': metrics['total_params'] / 1e6,
                'inference_time_ms': forward_stats['inference_time_ms'],
                'memory_mb': metrics['total_memory_mb']
            })
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = go.Figure()
        
        # æ•£ç‚¹å›¾ï¼šæ·±åº¦ vs å‚æ•°é‡
        fig.add_trace(go.Scatter(
            x=[r['n_layers'] for r in results],
            y=[r['params_millions'] for r in results],
            mode='markers+lines',
            name='å‚æ•°é‡ (M)',
            marker=dict(size=[r['d_model']/20 for r in results]),
            text=[r['name'] for r in results],
            hovertemplate='å±‚æ•°: %{x}<br>å‚æ•°é‡: %{y:.2f}M<br>æ¨¡å‹ç»´åº¦: %{marker.size:.0f}<br>%{text}<extra></extra>'
        ))
        
        fig.update_layout(
            title='æ·±åº¦ vs å®½åº¦æƒè¡¡åˆ†æ',
            xaxis_title='å±‚æ•° (æ·±åº¦)',
            yaxis_title='å‚æ•°é‡ (M)',
            height=500
        )
        
        return fig
    
    def create_parameter_recommendations(self, config: Dict) -> Dict:
        """åŸºäºå½“å‰é…ç½®ç”Ÿæˆå‚æ•°å»ºè®®"""
        recommendations = []
        
        # åˆ†æå‚æ•°é‡
        if config['d_model'] * config['n_layers'] > 10000:
            if config['n_heads'] > 16:
                recommendations.append({
                    'type': 'ä¼˜åŒ–å»ºè®®',
                    'message': f"æ¨¡å‹è¾ƒå¤§ï¼Œè€ƒè™‘å‡å°‘æ³¨æ„åŠ›å¤´æ•°åˆ° {config['d_model'] // 64} ä»¥æé«˜æ•ˆç‡",
                    'priority': 'Medium'
                })
        
        # åˆ†æåºåˆ—é•¿åº¦
        if config['seq_len'] > 1024:
            recommendations.append({
                'type': 'é•¿åºåˆ—ä¼˜åŒ–',
                'message': "åºåˆ—é•¿åº¦è¶…è¿‡1024ï¼Œè€ƒè™‘ä½¿ç”¨FlashAttentionæˆ–ç¨€ç–æ³¨æ„åŠ›",
                'priority': 'High'
            })
        
        # åˆ†ææ·±åº¦vså®½åº¦
        depth_to_width_ratio = config['n_layers'] / (config['d_model'] / 64)
        if depth_to_width_ratio > 2:
            recommendations.append({
                'type': 'æ¶æ„å¹³è¡¡',
                'message': "æ¨¡å‹ç›¸å¯¹è¾ƒæ·±ï¼Œè€ƒè™‘å¢åŠ ç»´åº¦ä»¥æ”¹å–„æ¢¯åº¦æµ",
                'priority': 'Low'
            })
        elif depth_to_width_ratio < 0.5:
            recommendations.append({
                'type': 'æ¶æ„å¹³è¡¡',
                'message': "æ¨¡å‹ç›¸å¯¹è¾ƒå®½ï¼Œè€ƒè™‘å¢åŠ å±‚æ•°ä»¥æé«˜è¡¨è¾¾èƒ½åŠ›",
                'priority': 'Low'
            })
        
        # Dropoutå»ºè®®
        if config['dropout'] > 0.2:
            recommendations.append({
                'type': 'æ­£åˆ™åŒ–',
                'message': "Dropoutè¾ƒé«˜ï¼Œå¯èƒ½å½±å“è®­ç»ƒé€Ÿåº¦ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®",
                'priority': 'Low'
            })
        elif config['dropout'] < 0.05 and config['n_layers'] > 6:
            recommendations.append({
                'type': 'æ­£åˆ™åŒ–',
                'message': "æ·±å±‚æ¨¡å‹å»ºè®®å¢åŠ Dropoutåˆ°0.1-0.15ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ",
                'priority': 'Medium'
            })
        
        return {
            'recommendations': recommendations,
            'config_summary': {
                'total_params_estimate': config['d_model'] * config['d_model'] * config['n_layers'] * 8 / 1e6,
                'complexity_level': 'High' if config['n_layers'] * config['d_model'] > 100000 else 'Medium' if config['n_layers'] * config['d_model'] > 50000 else 'Low'
            }
        }


def create_interactive_tuning_page():
    """åˆ›å»ºäº¤äº’å¼è°ƒèŠ‚é¡µé¢"""
    st.set_page_config(page_title="äº¤äº’å¼å‚æ•°è°ƒèŠ‚", page_icon="ğŸ›ï¸", layout="wide")
    
    st.title("ğŸ›ï¸ äº¤äº’å¼å‚æ•°è°ƒèŠ‚å·¥å…·")
    st.markdown("### å®æ—¶çœ‹åˆ°å‚æ•°å˜åŒ–å¯¹æ¨¡å‹çš„å½±å“")
    
    # åˆå§‹åŒ–è°ƒèŠ‚å™¨
    tuner = InteractiveParameterTuner()
    
    # ä¾§è¾¹æ å‚æ•°é…ç½®
    with st.sidebar:
        st.header("ğŸ”§ æ¨¡å‹å‚æ•°")
        
        d_model = st.slider("æ¨¡å‹ç»´åº¦ (d_model)", 64, 1024, 512, step=64)
        n_heads = st.selectbox("æ³¨æ„åŠ›å¤´æ•°", [2, 4, 8, 12, 16], index=2)
        n_layers = st.slider("å±‚æ•°", 1, 12, 6)
        activation = st.selectbox("æ¿€æ´»å‡½æ•°", ["relu", "gelu"], index=1)
        dropout = st.slider("Dropoutç‡", 0.0, 0.5, 0.1, step=0.05)
        seq_len = st.slider("åºåˆ—é•¿åº¦", 64, 2048, 128, step=64)
        
        st.divider()
        
        st.header("ğŸ“Š åˆ†æé€‰é¡¹")
        analysis_type = st.selectbox(
            "é€‰æ‹©åˆ†æç±»å‹",
            ["å‚æ•°å½±å“åˆ†æ", "æ³¨æ„åŠ›å¤´åˆ†æ", "æ·±åº¦vså®½åº¦åˆ†æ", "å®æ—¶æ€§èƒ½è¯„ä¼°"]
        )
    
    # å½“å‰é…ç½®
    current_config = {
        'd_model': d_model,
        'n_heads': n_heads,
        'n_layers': n_layers,
        'activation': activation,
        'dropout': dropout,
        'seq_len': seq_len
    }
    
    # åˆ›å»ºæ¨¡å‹å¹¶è®¡ç®—æŒ‡æ ‡
    model = tuner.create_simple_model(d_model, n_heads, n_layers, activation, dropout)
    metrics = tuner.calculate_model_metrics(model, seq_len)
    
    # æ˜¾ç¤ºå½“å‰é…ç½®æ‘˜è¦
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å‚æ•°é‡", f"{metrics['total_params']:,}")
    with col2:
        st.metric("è®¡ç®—é‡", f"{metrics['total_flops']/1e9:.2f} GFLOPs")
    with col3:
        st.metric("æ˜¾å­˜", f"{metrics['total_memory_mb']:.0f} MB")
    with col4:
        st.metric("æ¨ç†æ—¶é—´", f"{metrics['estimated_inference_time_ms']:.2f} ms")
    
    # æ ¹æ®åˆ†æç±»å‹æ˜¾ç¤ºä¸åŒå†…å®¹
    if analysis_type == "å‚æ•°å½±å“åˆ†æ":
        st.header("ğŸ“ˆ å‚æ•°å½±å“åˆ†æ")
        
        param_name = st.selectbox("é€‰æ‹©å‚æ•°", ["d_model", "n_layers", "seq_len", "dropout"])
        
        if param_name == "d_model":
            param_values = [128, 256, 384, 512, 640, 768, 896, 1024]
        elif param_name == "n_layers":
            param_values = [1, 2, 4, 6, 8, 10, 12]
        elif param_name == "seq_len":
            param_values = [64, 128, 256, 512, 1024, 2048]
        else:  # dropout
            param_values = [0.0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]
        
        fig = tuner.create_parameter_impact_visualization(param_name, param_values, current_config)
        st.plotly_chart(fig, use_container_width=True)
        
    elif analysis_type == "æ³¨æ„åŠ›å¤´åˆ†æ":
        st.header("ğŸ‘ï¸ æ³¨æ„åŠ›å¤´æ•°åˆ†æ")
        
        head_options = [2, 4, 8, 12, 16, 20, 24]
        valid_heads = [h for h in head_options if d_model % h == 0]
        
        if valid_heads:
            fig = tuner.create_attention_head_analysis(d_model, valid_heads)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning(f"å½“å‰ç»´åº¦ {d_model} æ— æ³•è¢«æ ‡å‡†å¤´æ•°æ•´é™¤ï¼Œå»ºè®®è°ƒæ•´ä¸º {d_model//8} æˆ– {d_model//16} ä¸ªå¤´")
        
    elif analysis_type == "æ·±åº¦vså®½åº¦åˆ†æ":
        st.header("ğŸ“Š æ·±åº¦ vs å®½åº¦æƒè¡¡")
        
        fig = tuner.create_depth_vs_width_analysis()
        st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("""
        **å…³é”®è§‚å¯Ÿ**ï¼š
        - **æµ…å±‚å®½æ¨¡å‹**ï¼šè®­ç»ƒå¿«ï¼Œå¹¶è¡Œæ€§å¥½ï¼Œé€‚åˆç®€å•ä»»åŠ¡
        - **æ·±å±‚çª„æ¨¡å‹**ï¼šè¡¨è¾¾èƒ½åŠ›å¼ºï¼Œå¯èƒ½æ¢¯åº¦æ¶ˆå¤±ï¼Œéœ€è¦ç²¾å¿ƒè®¾è®¡
        - **å¹³è¡¡ç‚¹**ï¼šé€šå¸¸å±‚æ•°å’Œç»´åº¦çš„æ¯”ä¾‹åœ¨ 1:4 åˆ° 1:8 ä¹‹é—´æ•ˆæœè¾ƒå¥½
        """)
        
    elif analysis_type == "å®æ—¶æ€§èƒ½è¯„ä¼°":
        st.header("âš¡ å®æ—¶æ€§èƒ½è¯„ä¼°")
        
        if st.button("è¿è¡Œå‰å‘ä¼ æ’­æµ‹è¯•"):
            with st.spinner("è¿è¡Œå‰å‘ä¼ æ’­..."):
                forward_stats = tuner.simulate_forward_pass(model, seq_len)
                
                st.success(f"å‰å‘ä¼ æ’­å®Œæˆï¼è€—æ—¶: {forward_stats['inference_time_ms']:.2f} ms")
                
                # æ˜¾ç¤ºå±‚ç»Ÿè®¡
                layer_stats = forward_stats['layer_stats']
                if layer_stats:
                    df_data = {
                        'å±‚æ•°': [s['layer'] for s in layer_stats],
                        'è¾“å‡ºèŒƒæ•°': [s['output_norm'] for s in layer_stats],
                        'è¾“å‡ºæ ‡å‡†å·®': [s['output_std'] for s in layer_stats]
                    }
                    
                    st.dataframe(df_data, use_container_width=True)
                    
                    # å¯è§†åŒ–å±‚ç»Ÿè®¡
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        x=[s['layer'] for s in layer_stats],
                        y=[s['output_norm'] for s in layer_stats],
                        mode='lines+markers',
                        name='è¾“å‡ºèŒƒæ•°'
                    ))
                    
                    fig.update_layout(
                        title='å„å±‚è¾“å‡ºèŒƒæ•°å˜åŒ–',
                        xaxis_title='å±‚æ•°',
                        yaxis_title='è¾“å‡ºèŒƒæ•°'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
    
    # å‚æ•°å»ºè®®
    st.divider()
    st.header("ğŸ’¡ æ™ºèƒ½å»ºè®®")
    
    recommendations = tuner.create_parameter_recommendations(current_config)
    
    if recommendations['recommendations']:
        for rec in recommendations['recommendations']:
            priority_color = {
                'High': 'ğŸ”´',
                'Medium': 'ğŸŸ¡', 
                'Low': 'ğŸŸ¢'
            }
            
            st.markdown(f"{priority_color.get(rec['priority'], 'âšª')} **{rec['type']}**: {rec['message']}")
    else:
        st.info("å½“å‰é…ç½®çœ‹èµ·æ¥å¾ˆåˆç†ï¼æ²¡æœ‰ç‰¹åˆ«çš„ä¼˜åŒ–å»ºè®®ã€‚")
    
    # é…ç½®æ‘˜è¦
    st.markdown(f"""
    **é…ç½®æ‘˜è¦**ï¼š
    - é¢„ä¼°å‚æ•°é‡: {recommendations['config_summary']['total_params_estimate']:.2f}M
    - å¤æ‚åº¦ç­‰çº§: {recommendations['config_summary']['complexity_level']}
    """)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    tuner = InteractiveParameterTuner()
    
    # åˆ›å»ºæ¨¡å‹
    model = tuner.create_simple_model(512, 8, 6, 'gelu', 0.1)
    metrics = tuner.calculate_model_metrics(model, 128)
    
    print(f"æ¨¡å‹å‚æ•°é‡: {metrics['total_params']:,}")
    print(f"è®¡ç®—é‡: {metrics['total_flops']/1e9:.2f} GFLOPs")
    
    # ç”Ÿæˆå»ºè®®
    config = {'d_model': 512, 'n_heads': 8, 'n_layers': 6, 'activation': 'gelu', 'dropout': 0.1, 'seq_len': 128}
    recommendations = tuner.create_parameter_recommendations(config)
    print(f"ç”Ÿæˆäº† {len(recommendations['recommendations'])} æ¡å»ºè®®")