"""
æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–å·¥å…·ï¼šå±•ç¤ºTransformeræ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List, Tuple, Optional
import string
import random


class AttentionVisualizer:
    """æ³¨æ„åŠ›æ¨¡å¼å¯è§†åŒ–å™¨"""
    
    def __init__(self, d_model: int = 512, n_heads: int = 8):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        
        # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
        self.model = self._create_sample_model()
        
        # ç¤ºä¾‹æ–‡æœ¬
        self.sample_texts = {
            "machine_translation": [
                "The cat sat on the mat",
                "çŒ« ååœ¨ å«å­ ä¸Š"
            ],
            "text_summarization": [
                "The quick brown fox jumps over the lazy dog and runs away",
                "ç‹ç‹¸è·³è¿‡æ‡’ç‹—"
            ],
            "question_answering": [
                "What is the capital of France? Paris is the capital of France",
                "æ³•å›½ çš„ é¦–éƒ½ æ˜¯ ä»€ä¹ˆ"
            ]
        }
    
    def _create_sample_model(self) -> nn.Module:
        """åˆ›å»ºç¤ºä¾‹Transformeræ¨¡å‹"""
        class SimpleAttention(nn.Module):
            def __init__(self, d_model, n_heads):
                super().__init__()
                self.d_model = d_model
                self.n_heads = n_heads
                self.head_dim = d_model // n_heads
                
                self.q_proj = nn.Linear(d_model, d_model)
                self.k_proj = nn.Linear(d_model, d_model)
                self.v_proj = nn.Linear(d_model, d_model)
                self.out_proj = nn.Linear(d_model, d_model)
                
            def forward(self, x):
                B, L, D = x.shape
                
                # è®¡ç®—Q, K, V
                Q = self.q_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
                K = self.k_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
                V = self.v_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)
                
                # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
                attn_weights = F.softmax(scores, dim=-1)
                
                # åº”ç”¨æ³¨æ„åŠ›
                attn_output = torch.matmul(attn_weights, V)
                attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)
                
                # è¾“å‡ºæŠ•å½±
                output = self.out_proj(attn_output)
                
                return output, attn_weights
        
        return SimpleAttention(self.d_model, self.n_heads)
    
    def generate_attention_patterns(self, text_type: str = "machine_translation") -> Dict:
        """ç”Ÿæˆä¸åŒç±»å‹çš„æ³¨æ„åŠ›æ¨¡å¼"""
        texts = self.sample_texts[text_type]
        
        # æ¨¡æ‹Ÿç¼–ç 
        tokens = texts[0].split()
        seq_len = len(tokens)
        
        # åˆ›å»ºéšæœºè¾“å…¥
        x = torch.randn(1, seq_len, self.d_model)
        
        # è·å–æ³¨æ„åŠ›æƒé‡
        with torch.no_grad():
            output, attn_weights = self.model(x)
        
        # æå–ä¸åŒå¤´çš„æ³¨æ„åŠ›æ¨¡å¼
        patterns = {}
        for head in range(self.n_heads):
            head_attn = attn_weights[0, head].cpu().numpy()
            
            # åˆ†ææ³¨æ„åŠ›æ¨¡å¼ç±»å‹
            pattern_type = self._classify_attention_pattern(head_attn, text_type)
            
            patterns[f"head_{head}"] = {
                "weights": head_attn,
                "pattern_type": pattern_type,
                "description": self._get_pattern_description(pattern_type)
            }
        
        return {
            "tokens": tokens,
            "patterns": patterns,
            "text_type": text_type
        }
    
    def _classify_attention_pattern(self, attn_matrix: np.ndarray, text_type: str) -> str:
        """åˆ†ç±»æ³¨æ„åŠ›æ¨¡å¼ç±»å‹"""
        # è®¡ç®—æ³¨æ„åŠ›ç‰¹å¾
        diagonal_strength = np.mean(np.diag(attn_matrix))
        max_off_diagonal = np.max(attn_matrix - np.diag(np.diag(attn_matrix)))
        entropy = -np.sum(attn_matrix * np.log(attn_matrix + 1e-8))
        
        if text_type == "machine_translation":
            if diagonal_strength > 0.5:
                return "diagonal_alignment"
            elif max_off_diagonal > 0.7:
                return "cross_attention"
            else:
                return "uniform_attention"
        
        elif text_type == "text_summarization":
            if entropy < 2.0:
                return "focused_attention"
            else:
                return "distributed_attention"
        
        elif text_type == "question_answering":
            if np.max(attn_matrix[:, 0]) > 0.8:  # ç¬¬ä¸€åˆ—ï¼ˆé—®é¢˜ï¼‰è¢«é«˜åº¦å…³æ³¨
                return "question_focus"
            else:
                return "answer_extraction"
        
        return "mixed_pattern"
    
    def _get_pattern_description(self, pattern_type: str) -> str:
        """è·å–æ¨¡å¼æè¿°"""
        descriptions = {
            "diagonal_alignment": "å¯¹é½æ¨¡å¼ - æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€è¯åºå¯¹åº”",
            "cross_attention": "äº¤å‰æ³¨æ„åŠ› - è·¨è¯­è¨€è¯å¯¹é½",
            "uniform_attention": "å‡åŒ€æ³¨æ„åŠ› - æ‰€æœ‰ä½ç½®å¹³ç­‰å…³æ³¨",
            "focused_attention": "èšç„¦æ³¨æ„åŠ› - å…³æ³¨å…³é”®ä¿¡æ¯",
            "distributed_attention": "åˆ†å¸ƒå¼æ³¨æ„åŠ› - ä¿¡æ¯åˆ†æ•£åœ¨å¤šå¤„",
            "question_focus": "é—®é¢˜èšç„¦ - é«˜åº¦å…³æ³¨é—®é¢˜éƒ¨åˆ†",
            "answer_extraction": "ç­”æ¡ˆæå– - ä»ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆ",
            "mixed_pattern": "æ··åˆæ¨¡å¼ - å¤šç§æ³¨æ„åŠ›æ¨¡å¼ç»„åˆ"
        }
        return descriptions.get(pattern_type, "æœªçŸ¥æ¨¡å¼")
    
    def visualize_attention_heatmap(self, head_idx: int = 0, text_type: str = "machine_translation") -> go.Figure:
        """å¯è§†åŒ–æ³¨æ„åŠ›çƒ­åŠ›å›¾"""
        data = self.generate_attention_patterns(text_type)
        tokens = data["tokens"]
        patterns = data["patterns"]
        
        if f"head_{head_idx}" not in patterns:
            return go.Figure()
        
        attn_weights = patterns[f"head_{head_idx}"]["weights"]
        pattern_type = patterns[f"head_{head_idx}"]["pattern_type"]
        description = patterns[f"head_{head_idx}"]["description"]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        fig = go.Figure(data=go.Heatmap(
            z=attn_weights,
            x=tokens,
            y=tokens,
            colorscale='Blues',
            showscale=True,
            hoverongaps=False,
            hovertemplate='ä» %{y} åˆ° %{x}<br>æ³¨æ„åŠ›æƒé‡: %{z:.3f}<extra></extra>'
        ))
        
        fig.update_layout(
            title=f'æ³¨æ„åŠ›æ¨¡å¼ - å¤´ {head_idx} ({pattern_type})<br>{description}',
            xaxis_title='ç›®æ ‡ä½ç½®',
            yaxis_title='æºä½ç½®',
            height=500,
            width=600
        )
        
        return fig
    
    def visualize_multi_head_attention(self, text_type: str = "machine_translation") -> go.Figure:
        """å¯è§†åŒ–å¤šå¤´æ³¨æ„åŠ›"""
        data = self.generate_attention_patterns(text_type)
        tokens = data["tokens"]
        patterns = data["patterns"]
        
        # åˆ›å»ºå­å›¾
        n_cols = 4
        n_rows = (self.n_heads + n_cols - 1) // n_cols
        
        fig = go.Figure()
        
        for head in range(self.n_heads):
            if f"head_{head}" not in patterns:
                continue
                
            attn_weights = patterns[f"head_{head}"]["weights"]
            pattern_type = patterns[f"head_{head}"]["pattern_type"]
            
            row = head // n_cols + 1
            col = head % n_cols + 1
            
            fig.add_trace(go.Heatmap(
                z=attn_weights,
                x=tokens,
                y=tokens,
                colorscale='Blues',
                showscale=False,
                name=f'Head {head}',
                hovertemplate=f'å¤´ {head}: %{{z:.3f}}<extra></extra>'
            ), row=row, col=col)
        
        fig.update_layout(
            title=f'å¤šå¤´æ³¨æ„åŠ›æ¨¡å¼ - {text_type}',
            height=300*n_rows,
            width=800,
            showlegend=False
        )
        
        # æ›´æ–°æ‰€æœ‰å­å›¾
        for i in range(1, self.n_heads + 1):
            fig.update_xaxes(title_text="ç›®æ ‡ä½ç½®", row=i, col=1)
            fig.update_yaxes(title_text="æºä½ç½®", row=i, col=1)
        
        return fig
    
    def create_attention_animation(self, text_type: str = "machine_translation") -> go.Figure:
        """åˆ›å»ºæ³¨æ„åŠ›æ¼”åŒ–åŠ¨ç”»"""
        data = self.generate_attention_patterns(text_type)
        tokens = data["tokens"]
        patterns = data["patterns"]
        
        # åˆ›å»ºåŠ¨ç”»å¸§
        frames = []
        for head in range(self.n_heads):
            if f"head_{head}" not in patterns:
                continue
                
            attn_weights = patterns[f"head_{head}"]["weights"]
            pattern_type = patterns[f"head_{head}"]["pattern_type"]
            
            frame = go.Frame(
                data=[go.Heatmap(
                    z=attn_weights,
                    x=tokens,
                    y=tokens,
                    colorscale='Blues',
                    showscale=True
                )],
                name=f'Head {head}'
            )
            frames.append(frame)
        
        # åˆ›å»ºåˆå§‹å›¾
        fig = go.Figure(
            data=[go.Heatmap(
                z=patterns["head_0"]["weights"],
                x=tokens,
                y=tokens,
                colorscale='Blues',
                showscale=True
            )],
            frames=frames
        )
        
        # æ·»åŠ æ’­æ”¾æŒ‰é’®
        fig.update_layout(
            title='æ³¨æ„åŠ›æ¨¡å¼åŠ¨ç”»',
            height=500,
            width=600,
            updatemenus=[{
                'type': 'buttons',
                'showactive': False,
                'buttons': [
                    {
                        'label': 'æ’­æ”¾',
                        'method': 'animate',
                        'args': [None, {
                            'frame': {'duration': 1000, 'redraw': True},
                            'fromcurrent': True,
                            'transition': {'duration': 500}
                        }]
                    },
                    {
                        'label': 'æš‚åœ',
                        'method': 'animate',
                        'args': [[None], {
                            'frame': {'duration': 0, 'redraw': False},
                            'mode': 'immediate',
                            'transition': {'duration': 0}
                        }]
                    }
                ]
            }]
        )
        
        return fig
    
    def analyze_attention_diversity(self, text_type: str = "machine_translation") -> Dict:
        """åˆ†ææ³¨æ„åŠ›å¤´çš„å¤šæ ·æ€§"""
        data = self.generate_attention_patterns(text_type)
        patterns = data["patterns"]
        
        # è®¡ç®—å¤´ä¹‹é—´çš„ç›¸ä¼¼åº¦
        similarities = {}
        for i in range(self.n_heads):
            for j in range(i+1, self.n_heads):
                if f"head_{i}" in patterns and f"head_{j}" in patterns:
                    attn_i = patterns[f"head_{i}"]["weights"].flatten()
                    attn_j = patterns[f"head_{j}"]["weights"].flatten()
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(attn_i, attn_j) / (np.linalg.norm(attn_i) * np.linalg.norm(attn_j) + 1e-8)
                    similarities[f"head_{i}_vs_head_{j}"] = similarity
        
        # åˆ†ææ¨¡å¼ç±»å‹åˆ†å¸ƒ
        pattern_counts = {}
        for head_data in patterns.values():
            pattern_type = head_data["pattern_type"]
            pattern_counts[pattern_type] = pattern_counts.get(pattern_type, 0) + 1
        
        return {
            "similarities": similarities,
            "pattern_distribution": pattern_counts,
            "diversity_score": len(pattern_counts) / self.n_heads
        }
    
    def create_attention_summary_report(self) -> str:
        """ç”Ÿæˆæ³¨æ„åŠ›åˆ†ææŠ¥å‘Š"""
        report = "# æ³¨æ„åŠ›æ¨¡å¼åˆ†ææŠ¥å‘Š\n\n"
        
        # åˆ†æä¸åŒä»»åŠ¡ç±»å‹çš„æ³¨æ„åŠ›æ¨¡å¼
        for task_type in self.sample_texts.keys():
            report += f"## {task_type.replace('_', ' ').title()} ä»»åŠ¡\n"
            
            diversity = self.analyze_attention_diversity(task_type)
            pattern_dist = diversity["pattern_distribution"]
            
            report += f"### æ¨¡å¼åˆ†å¸ƒ\n"
            for pattern, count in pattern_dist.items():
                report += f"- {pattern}: {count} ä¸ªå¤´\n"
            
            report += f"### å¤šæ ·æ€§è¯„åˆ†: {diversity['diversity_score']:.2f}\n"
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å¤´å¯¹
            similarities = diversity["similarities"]
            if similarities:
                most_similar = max(similarities.items(), key=lambda x: x[1])
                report += f"### æœ€ç›¸ä¼¼çš„å¤´å¯¹: {most_similar[0]} (ç›¸ä¼¼åº¦: {most_similar[1]:.3f})\n"
            
            report += "\n"
        
        # æ•™å­¦è¦ç‚¹
        report += """
## æ•™å­¦è¦ç‚¹

### ğŸ¯ æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ¦‚å¿µ
1. **å¤šå¤´æ³¨æ„åŠ›**: ä¸åŒçš„å¤´å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼
2. **ä»»åŠ¡ç‰¹å¼‚æ€§**: ä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„æ³¨æ„åŠ›ç­–ç•¥
3. **æ¨¡å¼å¤šæ ·æ€§**: é«˜å¤šæ ·æ€§é€šå¸¸æ„å‘³ç€æ¨¡å‹èƒ½æ•è·æ›´ä¸°å¯Œçš„ä¿¡æ¯

### ğŸ” å¸¸è§æ³¨æ„åŠ›æ¨¡å¼
- **å¯¹é½æ¨¡å¼**: ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¯å¯¹é½
- **èšç„¦æ¨¡å¼**: æ‘˜è¦ä»»åŠ¡ä¸­çš„å…³é”®ä¿¡æ¯æå–
- **é—®ç­”æ¨¡å¼**: é—®ç­”ä»»åŠ¡ä¸­çš„é—®é¢˜-ç­”æ¡ˆåŒ¹é…

### ğŸ’¡ ä¼˜åŒ–å»ºè®®
- å¦‚æœæ‰€æœ‰å¤´çš„æ¨¡å¼ç›¸ä¼¼ï¼Œè€ƒè™‘å¢åŠ å¤´çš„å¤šæ ·æ€§
- å¦‚æœæ³¨æ„åŠ›è¿‡äºåˆ†æ•£ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ¸©åº¦å‚æ•°
- ç‰¹å®šä»»åŠ¡å¯ä»¥è®¾è®¡ä¸“é—¨çš„æ³¨æ„åŠ›åç½®
"""
        
        return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    visualizer = AttentionVisualizer(d_model=512, n_heads=8)
    
    # ç”Ÿæˆæ³¨æ„åŠ›æ¨¡å¼
    patterns = visualizer.generate_attention_patterns("machine_translation")
    print(f"ç”Ÿæˆäº† {len(patterns['patterns'])} ä¸ªæ³¨æ„åŠ›å¤´çš„æ¨¡å¼")
    
    # åˆ†æå¤šæ ·æ€§
    diversity = visualizer.analyze_attention_diversity("machine_translation")
    print(f"æ³¨æ„åŠ›å¤šæ ·æ€§è¯„åˆ†: {diversity['diversity_score']:.2f}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = visualizer.create_attention_summary_report()
    print(report)
