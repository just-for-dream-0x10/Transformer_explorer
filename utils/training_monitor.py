"""
è®­ç»ƒåŠ¨æ€ç›‘æ§å·¥å…·ï¼šå®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import time
from collections import deque
import plotly.graph_objects as go
import plotly.express as px


@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡æ•°æ®"""
    step: int
    epoch: int
    loss: float
    learning_rate: float
    grad_norm: float
    param_norm: float
    throughput: float  # samples/second
    memory_usage: float  # MB


@dataclass
class LayerMetrics:
    """å±‚çº§æŒ‡æ ‡"""
    layer_name: str
    grad_norm: float
    param_norm: float
    update_ratio: float
    activation_sparsity: float
    dead_neurons_ratio: float


class TrainingMonitor:
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å™¨"""
    
    def __init__(self, model: nn.Module, window_size: int = 100):
        """
        Args:
            model: è¦ç›‘æ§çš„æ¨¡å‹
            window_size: æŒ‡æ ‡å†å²è®°å½•çš„çª—å£å¤§å°
        """
        self.model = model
        self.window_size = window_size
        
        # å†å²è®°å½•
        self.metrics_history = deque(maxlen=window_size)
        self.layer_metrics_history = deque(maxlen=window_size)
        
        # é’©å­æ³¨å†Œ
        self.hooks = []
        self.activations = {}
        self.gradients = {}
        
        # æ€§èƒ½è®¡æ—¶
        self.step_times = deque(maxlen=10)
        self.last_step_time = time.time()
        
    def register_hooks(self):
        """æ³¨å†Œå‰å‘å’Œåå‘ä¼ æ’­é’©å­"""
        def forward_hook(module, input, output):
            self.activations[module] = output.detach()
        
        def backward_hook(module, grad_input, grad_output):
            self.gradients[module] = grad_output[0].detach()
        
        # æ³¨å†Œé’©å­åˆ°æ‰€æœ‰å±‚
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Linear, nn.MultiheadAttention)):
                handle = module.register_forward_hook(forward_hook)
                self.hooks.append(handle)
                
                handle = module.register_backward_hook(backward_hook)
                self.hooks.append(handle)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for handle in self.hooks:
            handle.remove()
        self.hooks.clear()
    
    def compute_grad_norm(self) -> float:
        """è®¡ç®—å…¨å±€æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        return total_norm ** (1. / 2)
    
    def compute_param_norm(self) -> float:
        """è®¡ç®—å…¨å±€å‚æ•°èŒƒæ•°"""
        total_norm = 0
        for p in self.model.parameters():
            param_norm = p.data.norm(2)
            total_norm += param_norm.item() ** 2
        return total_norm ** (1. / 2)
    
    def analyze_layer_sparsity(self) -> List[LayerMetrics]:
        """åˆ†æå„å±‚çš„æ¿€æ´»ç¨€ç–æ€§å’Œæ¢¯åº¦"""
        layer_metrics = []
        
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Linear) and module in self.activations:
                # æ¿€æ´»ç¨€ç–æ€§
                activation = self.activations[module]
                sparsity = (activation.abs() < 1e-6).float().mean().item()
                
                # æ­»ç¥ç»å…ƒæ¯”ä¾‹ï¼ˆè¿ç»­å¤šæ­¥æ¥è¿‘é›¶ï¼‰
                dead_ratio = self._compute_dead_neurons_ratio(module, activation)
                
                # æ¢¯åº¦èŒƒæ•°
                grad_norm = 0
                if module in self.gradients:
                    grad_norm = self.gradients[module].norm(2).item()
                
                # å‚æ•°èŒƒæ•°
                param_norm = module.weight.data.norm(2).item()
                
                # æ›´æ–°æ¯”ä¾‹
                update_ratio = grad_norm / (param_norm + 1e-8)
                
                layer_metrics.append(LayerMetrics(
                    layer_name=name,
                    grad_norm=grad_norm,
                    param_norm=param_norm,
                    update_ratio=update_ratio,
                    activation_sparsity=sparsity,
                    dead_neurons_ratio=dead_ratio
                ))
        
        return layer_metrics
    
    def _compute_dead_neurons_ratio(self, module: nn.Module, activation: torch.Tensor) -> float:
        """è®¡ç®—æ­»ç¥ç»å…ƒæ¯”ä¾‹"""
        # ç®€åŒ–ç‰ˆï¼šç»Ÿè®¡ç»å¯¹å€¼å¾ˆå°çš„ç¥ç»å…ƒæ¯”ä¾‹
        # å®é™…åº”ç”¨ä¸­éœ€è¦è·Ÿè¸ªå¤šæ­¥çš„å†å²
        if len(activation.shape) > 2:
            # å¯¹äºå¤šç»´æ¿€æ´»ï¼Œå–æœ€åä¸€ä¸ªç»´åº¦
            dead_mask = activation.abs().mean(dim=tuple(range(len(activation.shape)-1))) < 1e-6
        else:
            dead_mask = activation.abs() < 1e-6
        
        return dead_mask.float().mean().item()
    
    def detect_anomalies(self) -> Dict[str, List[str]]:
        """æ£€æµ‹è®­ç»ƒå¼‚å¸¸"""
        anomalies = {
            'gradients': [],
            'activations': [],
            'parameters': [],
            'performance': []
        }
        
        if len(self.metrics_history) < 10:
            return anomalies
        
        recent_metrics = list(self.metrics_history)[-10:]
        
        # æ¢¯åº¦å¼‚å¸¸æ£€æµ‹
        grad_norms = [m.grad_norm for m in recent_metrics]
        if np.mean(grad_norms) < 1e-6:
            anomalies['gradients'].append("æ¢¯åº¦æ¶ˆå¤± - å¯èƒ½éœ€è¦è°ƒæ•´å­¦ä¹ ç‡æˆ–æ£€æŸ¥æ¨¡å‹ç»“æ„")
        elif np.mean(grad_norms) > 10:
            anomalies['gradients'].append("æ¢¯åº¦çˆ†ç‚¸ - è€ƒè™‘æ¢¯åº¦è£å‰ªæˆ–é™ä½å­¦ä¹ ç‡")
        
        # æ¢¯åº¦æ–¹å·®æ£€æµ‹
        if np.std(grad_norms) / (np.mean(grad_norms) + 1e-8) > 2:
            anomalies['gradients'].append("æ¢¯åº¦ä¸ç¨³å®š - è®­ç»ƒå¯èƒ½éœ‡è¡")
        
        # æ€§èƒ½å¼‚å¸¸æ£€æµ‹
        throughputs = [m.throughput for m in recent_metrics]
        if np.std(throughputs) / np.mean(throughputs) > 0.3:
            anomalies['performance'].append("ååé‡ä¸ç¨³å®š - å¯èƒ½å­˜åœ¨èµ„æºç«äº‰")
        
        # æŸå¤±å¼‚å¸¸æ£€æµ‹
        losses = [m.loss for m in recent_metrics]
        if np.isnan(losses).any():
            anomalies['parameters'].append("æŸå¤±ä¸º NaN - æ£€æŸ¥å­¦ä¹ ç‡å’Œæ•°å€¼ç¨³å®šæ€§")
        elif np.isinf(losses).any():
            anomalies['parameters'].append("æŸå¤±ä¸º Inf - å¯èƒ½å­˜åœ¨æ•°å€¼æº¢å‡º")
        
        # æ£€æµ‹æŸå¤±æ˜¯å¦åœæ­¢ä¸‹é™
        if len(losses) >= 5:
            recent_5 = losses[-5:]
            if np.std(recent_5) / (np.mean(recent_5) + 1e-8) < 0.01:
                anomalies['parameters'].append("æŸå¤±åœæ­¢ä¸‹é™ - å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜æˆ–éœ€è¦è°ƒæ•´å­¦ä¹ ç‡")
        
        return anomalies
    
    def step(self, step: int, epoch: int, loss: float, learning_rate: float, 
             batch_size: int) -> TrainingMetrics:
        """è®°å½•ä¸€æ­¥è®­ç»ƒçš„æŒ‡æ ‡"""
        current_time = time.time()
        step_time = current_time - self.last_step_time
        self.last_step_time = current_time
        
        # è®¡ç®—ååé‡
        throughput = batch_size / step_time if step_time > 0 else 0
        self.step_times.append(step_time)
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°å’Œå‚æ•°èŒƒæ•°
        grad_norm = self.compute_grad_norm()
        param_norm = self.compute_param_norm()
        
        # æ˜¾å­˜ä½¿ç”¨
        memory_usage = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
        
        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = TrainingMetrics(
            step=step,
            epoch=epoch,
            loss=loss,
            learning_rate=learning_rate,
            grad_norm=grad_norm,
            param_norm=param_norm,
            throughput=throughput,
            memory_usage=memory_usage
        )
        
        # è®°å½•å†å²
        self.metrics_history.append(metrics)
        
        # åˆ†æå±‚çº§æŒ‡æ ‡
        layer_metrics = self.analyze_layer_sparsity()
        self.layer_metrics_history.append(layer_metrics)
        
        return metrics
    
    def get_training_summary(self) -> Dict:
        """è·å–è®­ç»ƒæ‘˜è¦ç»Ÿè®¡"""
        if not self.metrics_history:
            return {}
        
        metrics = list(self.metrics_history)
        recent_metrics = metrics[-20:] if len(metrics) >= 20 else metrics
        
        return {
            'total_steps': len(metrics),
            'avg_loss': np.mean([m.loss for m in recent_metrics]),
            'avg_grad_norm': np.mean([m.grad_norm for m in recent_metrics]),
            'avg_throughput': np.mean([m.throughput for m in recent_metrics]),
            'avg_memory': np.mean([m.memory_usage for m in recent_metrics]),
            'loss_trend': 'decreasing' if len(recent_metrics) > 1 and recent_metrics[-1].loss < recent_metrics[0].loss else 'increasing',
            'training_stable': self._is_training_stable(recent_metrics)
        }
    
    def _is_training_stable(self, metrics: List[TrainingMetrics]) -> bool:
        """åˆ¤æ–­è®­ç»ƒæ˜¯å¦ç¨³å®š"""
        if len(metrics) < 5:
            return False
        
        losses = [m.loss for m in metrics]
        grad_norms = [m.grad_norm for m in metrics]
        
        # æŸå¤±å’Œæ¢¯åº¦éƒ½åº”è¯¥ç›¸å¯¹ç¨³å®š
        loss_stable = np.std(losses) / (np.mean(losses) + 1e-8) < 0.5
        grad_stable = np.std(grad_norms) / (np.mean(grad_norms) + 1e-8) < 1.0
        
        return loss_stable and grad_stable
    
    def visualize_training_curves(self) -> go.Figure:
        """å¯è§†åŒ–è®­ç»ƒæ›²çº¿"""
        if not self.metrics_history:
            return go.Figure()
        
        metrics = list(self.metrics_history)
        steps = [m.step for m in metrics]
        
        fig = go.Figure()
        
        # æŸå¤±æ›²çº¿
        fig.add_trace(go.Scatter(
            x=steps, y=[m.loss for m in metrics],
            mode='lines', name='Loss',
            line=dict(color='red', width=2)
        ))
        
        # æ¢¯åº¦èŒƒæ•°
        fig.add_trace(go.Scatter(
            x=steps, y=[m.grad_norm for m in metrics],
            mode='lines', name='Grad Norm',
            yaxis='y2',
            line=dict(color='blue', width=2)
        ))
        
        # è®¾ç½®åŒ y è½´
        fig.update_layout(
            title='è®­ç»ƒç›‘æ§æ›²çº¿',
            xaxis_title='Step',
            yaxis=dict(title='Loss', side='left'),
            yaxis2=dict(title='Grad Norm', side='right', overlaying='y'),
            height=400
        )
        
        return fig
    
    def visualize_layer_health(self) -> go.Figure:
        """å¯è§†åŒ–å±‚çº§å¥åº·çŠ¶å†µ"""
        if not self.layer_metrics_history:
            return go.Figure()
        
        # è·å–æœ€æ–°çš„å±‚çº§æŒ‡æ ‡
        latest_layer_metrics = self.layer_metrics_history[-1]
        
        layer_names = [m.layer_name for m in latest_layer_metrics]
        update_ratios = [m.update_ratio for m in latest_layer_metrics]
        sparsities = [m.activation_sparsity for m in latest_layer_metrics]
        
        fig = go.Figure()
        
        # æ›´æ–°æ¯”ä¾‹
        fig.add_trace(go.Bar(
            x=layer_names,
            y=update_ratios,
            name='Update Ratio',
            yaxis='y'
        ))
        
        # ç¨€ç–æ€§
        fig.add_trace(go.Bar(
            x=layer_names,
            y=sparsities,
            name='Activation Sparsity',
            yaxis='y2'
        ))
        
        fig.update_layout(
            title='å±‚çº§å¥åº·çŠ¶å†µ',
            xaxis_title='Layer',
            yaxis=dict(title='Update Ratio', side='left'),
            yaxis2=dict(title='Activation Sparsity', side='right', overlaying='y'),
            barmode='group',
            height=400
        )
        
        return fig


def create_training_report(monitor: TrainingMonitor) -> str:
    """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
    summary = monitor.get_training_summary()
    anomalies = monitor.detect_anomalies()
    
    report = f"""
# è®­ç»ƒç›‘æ§æŠ¥å‘Š

## ğŸ“Š è®­ç»ƒç»Ÿè®¡
- æ€»æ­¥æ•°: {summary.get('total_steps', 0)}
- å¹³å‡æŸå¤±: {summary.get('avg_loss', 0):.6f}
- å¹³å‡æ¢¯åº¦èŒƒæ•°: {summary.get('avg_grad_norm', 0):.6f}
- å¹³å‡ååé‡: {summary.get('avg_throughput', 0):.1f} samples/s
- å¹³å‡æ˜¾å­˜ä½¿ç”¨: {summary.get('avg_memory', 0):.1f} MB
- æŸå¤±è¶‹åŠ¿: {summary.get('loss_trend', 'unknown')}
- è®­ç»ƒç¨³å®šæ€§: {'âœ… ç¨³å®š' if summary.get('training_stable', False) else 'âš ï¸ ä¸ç¨³å®š'}

## âš ï¸ å¼‚å¸¸æ£€æµ‹
"""
    
    for category, issues in anomalies.items():
        if issues:
            report += f"\n### {category.title()}:\n"
            for issue in issues:
                report += f"- {issue}\n"
    
    if not any(anomalies.values()):
        report += "\nâœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸\n"
    
    return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from utils.model_profiler import create_sample_transformer
    
    model = create_sample_transformer()
    monitor = TrainingMonitor(model)
    monitor.register_hooks()
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for step in range(100):
        # æ¨¡æ‹ŸæŸå¤±ä¸‹é™
        loss = 10.0 * np.exp(-step * 0.01) + np.random.normal(0, 0.1)
        lr = 0.001 * (0.99 ** step)
        
        metrics = monitor.step(step, 0, loss, lr, batch_size=32)
        
        if step % 20 == 0:
            print(f"Step {step}: Loss={metrics.loss:.4f}, GradNorm={metrics.grad_norm:.4f}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = create_training_report(monitor)
    print(report)
    
    monitor.remove_hooks()
