"""
æ¨¡å‹æƒé‡åˆ†æå·¥å…·ï¼šåˆ†ææƒé‡åˆ†å¸ƒã€å¼‚å¸¸å€¼å’Œæ¼”åŒ–è¶‹åŠ¿
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import plotly.graph_objects as go
import plotly.express as px
from scipy import stats
import warnings
warnings.filterwarnings('ignore')


@dataclass
class WeightStats:
    """æƒé‡ç»Ÿè®¡ä¿¡æ¯"""
    layer_name: str
    mean: float
    std: float
    min: float
    max: float
    median: float
    skewness: float
    kurtosis: float
    outlier_ratio: float
    dead_ratio: float  # æ¥è¿‘é›¶çš„æƒé‡æ¯”ä¾‹


@dataclass
class WeightEvolution:
    """æƒé‡æ¼”åŒ–æ•°æ®"""
    layer_name: str
    step: int
    mean: float
    std: float
    norm: float
    update_magnitude: float


class WeightAnalyzer:
    """æ¨¡å‹æƒé‡åˆ†æå™¨"""
    
    def __init__(self, model: nn.Module):
        """
        Args:
            model: è¦åˆ†æçš„æ¨¡å‹
        """
        self.model = model
        self.weight_history: Dict[str, List[WeightEvolution]] = {}
        self.initial_weights: Dict[str, torch.Tensor] = {}
        
        # ä¿å­˜åˆå§‹æƒé‡
        self._save_initial_weights()
    
    def _save_initial_weights(self):
        """ä¿å­˜æ¨¡å‹åˆå§‹æƒé‡"""
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.initial_weights[name] = param.data.clone()
    
    def analyze_weight_distribution(self) -> List[WeightStats]:
        """åˆ†æå½“å‰æƒé‡åˆ†å¸ƒ"""
        weight_stats = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.dim() >= 2:  # åªåˆ†ææƒé‡çŸ©é˜µ
                weights = param.data.flatten().cpu().numpy()
                
                # åŸºæœ¬ç»Ÿè®¡
                mean = np.mean(weights)
                std = np.std(weights)
                min_val = np.min(weights)
                max_val = np.max(weights)
                median = np.median(weights)
                
                # é«˜é˜¶ç»Ÿè®¡
                skewness = stats.skew(weights)
                kurtosis = stats.kurtosis(weights)
                
                # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆ3ÏƒåŸåˆ™ï¼‰
                outlier_mask = np.abs(weights - mean) > 3 * std
                outlier_ratio = np.mean(outlier_mask)
                
                # æ­»æƒé‡æ£€æµ‹ï¼ˆæ¥è¿‘é›¶ï¼‰
                dead_mask = np.abs(weights) < 1e-6
                dead_ratio = np.mean(dead_mask)
                
                weight_stats.append(WeightStats(
                    layer_name=name,
                    mean=mean,
                    std=std,
                    min=min_val,
                    max=max_val,
                    median=median,
                    skewness=skewness,
                    kurtosis=kurtosis,
                    outlier_ratio=outlier_ratio,
                    dead_ratio=dead_ratio
                ))
        
        return weight_stats
    
    def record_weight_evolution(self, step: int):
        """è®°å½•æƒé‡æ¼”åŒ–"""
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.dim() >= 2:
                weights = param.data
                
                if name not in self.weight_history:
                    self.weight_history[name] = []
                
                # è®¡ç®—æ›´æ–°å¹…åº¦
                if name in self.initial_weights:
                    update_magnitude = (weights - self.initial_weights[name]).norm().item()
                else:
                    update_magnitude = 0
                
                evolution = WeightEvolution(
                    layer_name=name,
                    step=step,
                    mean=weights.mean().item(),
                    std=weights.std().item(),
                    norm=weights.norm().item(),
                    update_magnitude=update_magnitude
                )
                
                self.weight_history[name].append(evolution)
    
    def detect_weight_anomalies(self) -> Dict[str, List[str]]:
        """æ£€æµ‹æƒé‡å¼‚å¸¸"""
        anomalies = {}
        weight_stats = self.analyze_weight_distribution()
        
        for stats in weight_stats:
            layer_anomalies = []
            
            # æ£€æµ‹æƒé‡æ¶ˆå¤±
            if abs(stats.mean) < 1e-6 and stats.std < 1e-6:
                layer_anomalies.append("æƒé‡æ¶ˆå¤± - å±‚å¯èƒ½æœªå‚ä¸è®­ç»ƒ")
            
            # æ£€æµ‹æƒé‡çˆ†ç‚¸
            if stats.std > 10 or abs(stats.mean) > 10:
                layer_anomalies.append("æƒé‡çˆ†ç‚¸ - å¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸é—®é¢˜")
            
            # æ£€æµ‹å¼‚å¸¸å€¼è¿‡å¤š
            if stats.outlier_ratio > 0.05:
                layer_anomalies.append("å¼‚å¸¸å€¼è¿‡å¤š - å¯èƒ½å­˜åœ¨æ•°å€¼ä¸ç¨³å®š")
            
            # æ£€æµ‹æ­»æƒé‡è¿‡å¤š
            if stats.dead_ratio > 0.5:
                layer_anomalies.append("æ­»æƒé‡è¿‡å¤š - å±‚å¯èƒ½è¿‡äºç¨€ç–")
            
            # æ£€æµ‹åˆ†å¸ƒåæ–œ
            if abs(stats.skewness) > 2:
                layer_anomalies.append("æƒé‡åˆ†å¸ƒä¸¥é‡åæ–œ")
            
            # æ£€æµ‹å³°åº¦å¼‚å¸¸
            if abs(stats.kurtosis) > 10:
                layer_anomalies.append("æƒé‡å³°åº¦å¼‚å¸¸ - å¯èƒ½å­˜åœ¨æç«¯å€¼")
            
            if layer_anomalies:
                anomalies[stats.layer_name] = layer_anomalies
        
        return anomalies
    
    def compare_initialization_methods(self, layer_name: str, 
                                     init_methods: List[str]) -> Dict:
        """æ¯”è¾ƒä¸åŒåˆå§‹åŒ–æ–¹æ³•çš„æ•ˆæœ"""
        if layer_name not in self.initial_weights:
            return {}
        
        original_weights = self.initial_weights[layer_name]
        results = {}
        
        for method in init_methods:
            # å¤åˆ¶åŸå§‹æƒé‡
            test_weights = original_weights.clone()
            
            # åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–
            if method == "xavier_uniform":
                nn.init.xavier_uniform_(test_weights)
            elif method == "xavier_normal":
                nn.init.xavier_normal_(test_weights)
            elif method == "kaiming_uniform":
                nn.init.kaiming_uniform_(test_weights, nonlinearity='relu')
            elif method == "kaiming_normal":
                nn.init.kaiming_normal_(test_weights, nonlinearity='relu')
            elif method == "orthogonal":
                nn.init.orthogonal_(test_weights)
            else:
                continue
            
            # åˆ†æåˆå§‹åŒ–åçš„åˆ†å¸ƒ
            weights_np = test_weights.flatten().cpu().numpy()
            results[method] = {
                'mean': np.mean(weights_np),
                'std': np.std(weights_np),
                'frobenius_norm': test_weights.norm().item(),
                'condition_number': self._estimate_condition_number(test_weights)
            }
        
        return results
    
    def _estimate_condition_number(self, weight_matrix: torch.Tensor) -> float:
        """ä¼°ç®—çŸ©é˜µæ¡ä»¶æ•°"""
        try:
            # å¯¹äºå¤§çŸ©é˜µï¼Œä½¿ç”¨éšæœºé‡‡æ ·ä¼°ç®—
            if weight_matrix.numel() > 10000:
                # éšæœºé€‰æ‹©éƒ¨åˆ†è¡Œå’Œåˆ—
                m, n = weight_matrix.shape
                sample_size = min(100, min(m, n))
                rows_idx = torch.randperm(m)[:sample_size]
                cols_idx = torch.randperm(n)[:sample_size]
                sampled = weight_matrix[rows_idx][:, cols_idx]
            else:
                sampled = weight_matrix
            
            # è®¡ç®—å¥‡å¼‚å€¼
            singular_values = torch.linalg.svdvals(sampled.float())
            if len(singular_values) > 0 and singular_values[-1] > 1e-10:
                return (singular_values[0] / singular_values[-1]).item()
            else:
                return float('inf')
        except:
            return float('inf')
    
    def analyze_weight_correlation(self) -> Dict[str, Dict[str, float]]:
        """åˆ†æå±‚é—´æƒé‡ç›¸å…³æ€§"""
        layer_weights = {}
        
        # æ”¶é›†å„å±‚çš„æƒé‡
        for name, param in self.model.named_parameters():
            if param.requires_grad and param.dim() >= 2:
                # å±•å¹³æƒé‡å‘é‡
                layer_weights[name] = param.data.flatten().cpu().numpy()
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
        layer_names = list(layer_weights.keys())
        correlations = {}
        
        for i, name1 in enumerate(layer_names):
            correlations[name1] = {}
            for j, name2 in enumerate(layer_names):
                if i <= j:  # åªè®¡ç®—ä¸Šä¸‰è§’
                    corr = np.corrcoef(layer_weights[name1], layer_weights[name2])[0, 1]
                    correlations[name1][name2] = corr if not np.isnan(corr) else 0.0
                else:
                    correlations[name1][name2] = correlations[name2][name1]
        
        return correlations
    
    def visualize_weight_distribution(self, layer_name: str = None) -> go.Figure:
        """å¯è§†åŒ–æƒé‡åˆ†å¸ƒ"""
        weight_stats = self.analyze_weight_distribution()
        
        if layer_name:
            # å•å±‚åˆ†å¸ƒ
            stats = next((s for s in weight_stats if s.layer_name == layer_name), None)
            if not stats:
                return go.Figure()
            
            param = next((p for n, p in self.model.named_parameters() 
                         if n == layer_name and p.requires_grad), None)
            if param is None:
                return go.Figure()
            
            weights = param.data.flatten().cpu().numpy()
            
            fig = go.Figure()
            
            # ç›´æ–¹å›¾
            fig.add_trace(go.Histogram(
                x=weights,
                nbinsx=100,
                name='æƒé‡åˆ†å¸ƒ',
                histnorm='probability density'
            ))
            
            # æ·»åŠ ç»Ÿè®¡çº¿
            fig.add_vline(x=stats.mean, line_dash="dash", line_color="red", 
                         annotation_text=f"å‡å€¼: {stats.mean:.4f}")
            fig.add_vline(x=stats.median, line_dash="dash", line_color="green",
                         annotation_text=f"ä¸­ä½æ•°: {stats.median:.4f}")
            
            fig.update_layout(
                title=f'{layer_name} æƒé‡åˆ†å¸ƒ',
                xaxis_title='æƒé‡å€¼',
                yaxis_title='å¯†åº¦',
                height=400
            )
            
        else:
            # å¤šå±‚åˆ†å¸ƒå¯¹æ¯”
            fig = go.Figure()
            
            for stats in weight_stats[:10]:  # åªæ˜¾ç¤ºå‰10å±‚
                param = next((p for n, p in self.model.named_parameters() 
                            if n == stats.layer_name and p.requires_grad), None)
                if param is not None:
                    weights = param.data.flatten().cpu().numpy()
                    
                    fig.add_trace(go.Histogram(
                        x=weights,
                        nbinsx=50,
                        name=stats.layer_name.split('.')[-1],
                        histnorm='probability density',
                        opacity=0.7
                    ))
            
            fig.update_layout(
                title='å„å±‚æƒé‡åˆ†å¸ƒå¯¹æ¯”',
                xaxis_title='æƒé‡å€¼',
                yaxis_title='å¯†åº¦',
                height=500,
                barmode='overlay'
            )
        
        return fig
    
    def visualize_weight_evolution(self, layer_name: str) -> go.Figure:
        """å¯è§†åŒ–æƒé‡æ¼”åŒ–"""
        if layer_name not in self.weight_history:
            return go.Figure()
        
        history = self.weight_history[layer_name]
        steps = [h.step for h in history]
        means = [h.mean for h in history]
        stds = [h.std for h in history]
        norms = [h.norm for h in history]
        
        fig = go.Figure()
        
        # å‡å€¼æ¼”åŒ–
        fig.add_trace(go.Scatter(
            x=steps, y=means,
            mode='lines+markers',
            name='å‡å€¼',
            line=dict(color='blue')
        ))
        
        # æ ‡å‡†å·®æ¼”åŒ–
        fig.add_trace(go.Scatter(
            x=steps, y=stds,
            mode='lines+markers',
            name='æ ‡å‡†å·®',
            yaxis='y2',
            line=dict(color='red')
        ))
        
        # èŒƒæ•°æ¼”åŒ–
        fig.add_trace(go.Scatter(
            x=steps, y=norms,
            mode='lines+markers',
            name='FrobeniusèŒƒæ•°',
            yaxis='y3',
            line=dict(color='green')
        ))
        
        fig.update_layout(
            title=f'{layer_name} æƒé‡æ¼”åŒ–',
            xaxis_title='è®­ç»ƒæ­¥æ•°',
            yaxis=dict(title='å‡å€¼', side='left'),
            yaxis2=dict(title='æ ‡å‡†å·®', side='right', overlaying='y'),
            yaxis3=dict(title='èŒƒæ•°', side='right', overlaying='y', position=0.85),
            height=400
        )
        
        return fig
    
    def generate_weight_report(self) -> str:
        """ç”Ÿæˆæƒé‡åˆ†ææŠ¥å‘Š"""
        weight_stats = self.analyze_weight_distribution()
        anomalies = self.detect_weight_anomalies()
        correlations = self.analyze_weight_correlation()
        
        report = """
# æ¨¡å‹æƒé‡åˆ†ææŠ¥å‘Š

## ğŸ“Š æƒé‡ç»Ÿè®¡æ‘˜è¦
"""
        
        # æ•´ä½“ç»Ÿè®¡
        all_means = [s.mean for s in weight_stats]
        all_stds = [s.std for s in weight_stats]
        all_outliers = [s.outlier_ratio for s in weight_stats]
        all_deads = [s.dead_ratio for s in weight_stats]
        
        report += f"""
- å¹³å‡æƒé‡èŒƒå›´: [{np.min(all_means):.6f}, {np.max(all_means):.6f}]
- å¹³å‡æ ‡å‡†å·®: {np.mean(all_stds):.6f}
- å¹³å‡å¼‚å¸¸å€¼æ¯”ä¾‹: {np.mean(all_outliers)*100:.2f}%
- å¹³å‡æ­»æƒé‡æ¯”ä¾‹: {np.mean(all_deads)*100:.2f}%
"""
        
        # å¼‚å¸¸æŠ¥å‘Š
        if anomalies:
            report += "\n## âš ï¸ æƒé‡å¼‚å¸¸\n"
            for layer_name, issues in anomalies.items():
                report += f"\n### {layer_name}:\n"
                for issue in issues:
                    report += f"- {issue}\n"
        else:
            report += "\n## âœ… æœªæ£€æµ‹åˆ°æƒé‡å¼‚å¸¸\n"
        
        # ç›¸å…³æ€§åˆ†æ
        report += "\n## ğŸ”„ å±‚é—´ç›¸å…³æ€§åˆ†æ\n"
        
        # æ‰¾å‡ºç›¸å…³æ€§æœ€é«˜å’Œæœ€ä½çš„å±‚å¯¹
        high_corr_pairs = []
        low_corr_pairs = []
        
        for i, layer1 in enumerate(correlations):
            for j, layer2 in enumerate(correlations[layer1]):
                if i < j:  # é¿å…é‡å¤
                    corr = correlations[layer1][layer2]
                    if corr > 0.5:
                        high_corr_pairs.append((layer1, layer2, corr))
                    elif corr < -0.5:
                        low_corr_pairs.append((layer1, layer2, corr))
        
        if high_corr_pairs:
            report += "\n### é«˜ç›¸å…³æ€§å±‚å¯¹ (>0.5):\n"
            for layer1, layer2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True)[:5]:
                report += f"- {layer1} â†” {layer2}: {corr:.3f}\n"
        
        if low_corr_pairs:
            report += "\n### ä½ç›¸å…³æ€§å±‚å¯¹ (<-0.5):\n"
            for layer1, layer2, corr in sorted(low_corr_pairs, key=lambda x: x[2])[:5]:
                report += f"- {layer1} â†” {layer2}: {corr:.3f}\n"
        
        # ä¼˜åŒ–å»ºè®®
        report += "\n## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n"
        
        if np.mean(all_outliers) > 0.05:
            report += "- è€ƒè™‘ä½¿ç”¨æ¢¯åº¦è£å‰ªæ¥å‡å°‘å¼‚å¸¸å€¼\n"
        
        if np.mean(all_deads) > 0.3:
            report += "- è€ƒè™‘å¢åŠ å­¦ä¹ ç‡æˆ–æ£€æŸ¥æƒé‡åˆå§‹åŒ–\n"
        
        if np.mean(all_stds) > 5:
            report += "- æƒé‡æ–¹å·®è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–æ–¹æ³•\n"
        
        if np.mean(all_stds) < 0.01:
            report += "- æƒé‡æ–¹å·®è¿‡å°ï¼Œå¯èƒ½éœ€è¦å¢åŠ å­¦ä¹ ç‡\n"
        
        return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from utils.model_profiler import create_sample_transformer
    
    model = create_sample_transformer()
    analyzer = WeightAnalyzer(model)
    
    # åˆ†ææƒé‡
    stats = analyzer.analyze_weight_distribution()
    print(f"åˆ†æäº† {len(stats)} å±‚çš„æƒé‡åˆ†å¸ƒ")
    
    # æ£€æµ‹å¼‚å¸¸
    anomalies = analyzer.detect_weight_anomalies()
    print(f"å‘ç° {len(anomalies)} å±‚å­˜åœ¨å¼‚å¸¸")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for step in range(10):
        # æ¨¡æ‹Ÿæƒé‡æ›´æ–°
        with torch.no_grad():
            for param in model.parameters():
                if param.requires_grad:
                    param.data += torch.randn_like(param.data) * 0.001
        
        analyzer.record_weight_evolution(step)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_weight_report()
    print(report)
